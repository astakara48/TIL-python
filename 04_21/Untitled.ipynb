{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist 분류\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True) # MNIST_data 위치에 저장, one_hot 되어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10]) # mnist에서 0~9까지 총 10가지 숫자가 나올수 있기 때문에 10으로 줘야함\n",
    "w=tf.Variable(tf.random_normal([784,10]))\n",
    "b=tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.nn.softmax(tf.matmul(x,w)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCorrect=tf.equal(tf.argmax(hf,1), tf.argmax(y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=tf.reduce_mean(tf.cast(isCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs=30\n",
    "batchSize=100\n",
    "numIter=int(mnist.train.num_examples/batchSize)\n",
    "# 60000 / 100 = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭:0001, cost:2.664012645\n",
      "에폭:0002, cost:1.122327304\n",
      "에폭:0003, cost:0.888809580\n",
      "에폭:0004, cost:0.776854574\n",
      "에폭:0005, cost:0.707442603\n",
      "에폭:0006, cost:0.658287961\n",
      "에폭:0007, cost:0.620930457\n",
      "에폭:0008, cost:0.590861757\n",
      "에폭:0009, cost:0.565623356\n",
      "에폭:0010, cost:0.545144398\n",
      "에폭:0011, cost:0.526915397\n",
      "에폭:0012, cost:0.510827083\n",
      "에폭:0013, cost:0.497255454\n",
      "에폭:0014, cost:0.485231210\n",
      "에폭:0015, cost:0.473714150\n",
      "에폭:0016, cost:0.463473346\n",
      "에폭:0017, cost:0.454809998\n",
      "에폭:0018, cost:0.446180601\n",
      "에폭:0019, cost:0.438572190\n",
      "에폭:0020, cost:0.431376680\n",
      "에폭:0021, cost:0.425056491\n",
      "에폭:0022, cost:0.418879810\n",
      "에폭:0023, cost:0.412906861\n",
      "에폭:0024, cost:0.407507458\n",
      "에폭:0025, cost:0.402212210\n",
      "에폭:0026, cost:0.397720792\n",
      "에폭:0027, cost:0.393101391\n",
      "에폭:0028, cost:0.388620468\n",
      "에폭:0029, cost:0.384340132\n",
      "에폭:0030, cost:0.380916618\n",
      "정확도: 0.8987\n",
      "레이블: [8]\n",
      "예측: [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 트레이닝\n",
    "    for epoch in range(numEpochs): # 15 epochs\n",
    "        avgCv=0\n",
    "        for i in range(numIter): # 600\n",
    "            batchX, batchY=mnist.train.next_batch(batchSize) # 한번에 몇개의 data를 읽어올지\n",
    "            _,cv=sess.run([train, cost],feed_dict={x:batchX, y:batchY})\n",
    "            avgCv+=cv/numIter\n",
    "        print(\"에폭:{:04d}, cost:{:.9f}\".format(epoch+1, avgCv))  #{04d} 4자리 십진수\n",
    "    print(\"정확도:\",accuracy.eval(session=sess, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "    \n",
    "    r=random.randint(0,mnist.test.num_examples-1)\n",
    "    print(\"레이블:\",sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "    print(\"예측:\",sess.run(tf.argmax(hf,1),feed_dict={x:mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap=\"Greys\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 저장 / 불러오기 (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론 모델\n",
    "# 훈련셋, 검증셋, 실험셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, yTrain), (xTest, yTest)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=xTrain.reshape(60000,784).astype(\"float32\")/255.0\n",
    "xTest=xTest.reshape(10000,784).astype(\"float32\")/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain=np_utils.to_categorical(yTrain) # ohe \n",
    "yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest=np_utils.to_categorical(yTest) # ohe \n",
    "yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVal=xTrain[42000:]\n",
    "xTrain=xTrain[:42000]\n",
    "yVal=yTrain[42000:]\n",
    "yTrain=yTrain[:42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "model=Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 1.0130 - accuracy: 0.7485 - val_loss: 0.5164 - val_accuracy: 0.8683\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.4518 - accuracy: 0.8809 - val_loss: 0.3884 - val_accuracy: 0.8932\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3733 - accuracy: 0.8971 - val_loss: 0.3450 - val_accuracy: 0.9013\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3368 - accuracy: 0.9055 - val_loss: 0.3198 - val_accuracy: 0.9077\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3134 - accuracy: 0.9126 - val_loss: 0.3023 - val_accuracy: 0.9132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2ea0205f8c8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 환결 설정(compile)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# 학습(fit)\n",
    "model.fit(xTrain,yTrain, epochs=5, batch_size=50, validation_data=(xVal, yVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\n",
      "평가결과:[0.2894299391284585, 0.9197999835014343]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기(test data)\n",
    "metrics=model.evaluate(xTest, yTest, batch_size=50)\n",
    "print(\"평가결과:\"+str(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(xTest.shape[0],5)\n",
    "xHat=xTest[idx]\n",
    "yHat=model.predict_classes(xHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [3 8 8 4 9]\n",
      "예측값:3 실제값:3\n",
      "예측값:8 실제값:8\n",
      "예측값:8 실제값:8\n",
      "예측값:4 실제값:4\n",
      "예측값:9 실제값:9\n"
     ]
    }
   ],
   "source": [
    "xHat\n",
    "print(\"예측값:\",yHat)\n",
    "for i in range(5):\n",
    "    print(\"예측값:\"+str(yHat[i])+\" 실제값:\"+str(np.argmax(yTest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n모델 : 모델 아키텍쳐와 모델 가중치로 구성\\n모델 아키텍쳐 : 모델이 어떤 층으로 구성\\n모델 가중치 : weight, bias\\n\\nsave : 케라스 모델 저장 함수(아키텍처+가중치)\\n파일형식 : h5로 저장\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "모델 : 모델 아키텍쳐와 모델 가중치로 구성\n",
    "모델 아키텍쳐 : 모델이 어떤 층으로 구성\n",
    "모델 가중치 : weight, bias\n",
    "\n",
    "save : 케라스 모델 저장 함수(아키텍처+가중치)\n",
    "파일형식 : h5로 저장\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 아키텍쳐 확인\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from IPython.display import SVG\n",
    "# SVG(model_to_dot(model, show_shapes=True), create(prog='dot', format='svg'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [1 4 5 5 9 3 0 6 0 1]\n",
      "예측값:1 실제값:1\n",
      "예측값:4 실제값:4\n",
      "예측값:5 실제값:5\n",
      "예측값:5 실제값:5\n",
      "예측값:9 실제값:9\n",
      "예측값:3 실제값:8\n",
      "예측값:0 실제값:0\n",
      "예측값:6 실제값:6\n",
      "예측값:0 실제값:0\n",
      "예측값:1 실제값:1\n"
     ]
    }
   ],
   "source": [
    "# 실제 데이터 사용\n",
    "(xTrain, yTrain),(xTest,yTest)=mnist.load_data()\n",
    "xTest=xTest.reshape(10000,784).astype(\"float32\")\n",
    "yTest=np_utils.to_categorical(yTest)\n",
    "idx=np.random.choice(xTest.shape[0],10)\n",
    "xhat=xTest[idx]\n",
    "# 모델 불러오기\n",
    "from keras.models import load_model\n",
    "model=load_model(\"mnist_model.h5\")\n",
    "yhat=model.predict_classes(xhat)\n",
    "\n",
    "print(\"예측값:\",yhat)\n",
    "for i in range(10):\n",
    "    print(\"예측값:\"+str(yhat[i])+\" 실제값:\"+str(np.argmax(yTest[idx[i]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=xy[:,:4]\n",
    "ydata=xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[831.659973],\n",
       "       [828.070007],\n",
       "       [824.159973],\n",
       "       [819.23999 ],\n",
       "       [818.97998 ],\n",
       "       [820.450012],\n",
       "       [813.669983],\n",
       "       [809.559998]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,4])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(y-hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 539657830000.0 \n",
      "Prediction: [[ 518633.06]\n",
      " [1044459.94]\n",
      " [ 821558.06]\n",
      " [ 575793.6 ]\n",
      " [ 678671.94]\n",
      " [ 684386.7 ]\n",
      " [ 627235.94]\n",
      " [ 798697.4 ]]\n",
      "1 cost: 5.929114e+26 \n",
      "Prediction: [[-1.7176166e+13]\n",
      " [-3.4577381e+13]\n",
      " [-2.7200779e+13]\n",
      " [-1.9067602e+13]\n",
      " [-2.2472187e+13]\n",
      " [-2.2661332e+13]\n",
      " [-2.0769896e+13]\n",
      " [-2.6444202e+13]]\n",
      "2 cost: inf \n",
      "Prediction: [[5.6932719e+20]\n",
      " [1.1461140e+21]\n",
      " [9.0160657e+20]\n",
      " [6.3202145e+20]\n",
      " [7.4487103e+20]\n",
      " [7.5114046e+20]\n",
      " [6.8844613e+20]\n",
      " [8.7652884e+20]]\n",
      "3 cost: inf \n",
      "Prediction: [[-1.8871117e+28]\n",
      " [-3.7989493e+28]\n",
      " [-2.9884963e+28]\n",
      " [-2.0949202e+28]\n",
      " [-2.4689753e+28]\n",
      " [-2.4897560e+28]\n",
      " [-2.2819476e+28]\n",
      " [-2.9053729e+28]]\n",
      "4 cost: inf \n",
      "Prediction: [[6.2550856e+35]\n",
      " [1.2592129e+36]\n",
      " [9.9057736e+35]\n",
      " [6.9438945e+35]\n",
      " [8.1837510e+35]\n",
      " [8.2526320e+35]\n",
      " [7.5638224e+35]\n",
      " [9.6302497e+35]]\n",
      "5 cost: inf \n",
      "Prediction: [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(101):\n",
    "        cv,hv,_=sess.run([cost, hf, train], feed_dict={x:xdata,y:ydata})\n",
    "        print(step,\"cost:\",cv,\"\\nPrediction:\",hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMinMax(data):\n",
    "    #print(np.min(data))\n",
    "    #print(np.min(data, axis=1)) # 행 단위로 최소값 출력\n",
    "    #print(np.min(data, axis=0)) # 열 단위로 최소값 출력\n",
    "    \n",
    "    bm=np.max(data,0)-np.min(data,0)\n",
    "    bj=data-np.mean(data)\n",
    "    \n",
    "    return bj/bm\n",
    "    \n",
    "xy=myMinMax(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.31304231e+04, -1.38155033e+04,  7.12852418e-01,\n",
       "        -1.05605816e+04, -1.13775747e+04],\n",
       "       [-1.31307176e+04, -1.38157989e+04,  1.71285242e+00,\n",
       "        -1.05608627e+04, -1.13777371e+04],\n",
       "       [-1.31308789e+04, -1.38160005e+04,  1.28893937e+00,\n",
       "        -1.05609751e+04, -1.13779140e+04],\n",
       "       [-1.31310841e+04, -1.38161896e+04,  8.21548070e-01,\n",
       "        -1.05611217e+04, -1.13781367e+04],\n",
       "       [-1.31309087e+04, -1.38160775e+04,  1.01720024e+00,\n",
       "        -1.05609965e+04, -1.13781484e+04],\n",
       "       [-1.31309275e+04, -1.38160775e+04,  1.02806981e+00,\n",
       "        -1.05611002e+04, -1.13780819e+04],\n",
       "       [-1.31313087e+04, -1.38165033e+04,  9.19374157e-01,\n",
       "        -1.05613615e+04, -1.13783887e+04],\n",
       "       [-1.31314231e+04, -1.38164258e+04,  1.24546111e+00,\n",
       "        -1.05615816e+04, -1.13785747e+04]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=xy[:,0:-1]\n",
    "ydata=xy[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,4])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([4,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(y-hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 869900300.0 \n",
      "Prediction: [[18115.633]\n",
      " [18114.984]\n",
      " [18115.648]\n",
      " [18116.434]\n",
      " [18115.975]\n",
      " [18116.064]\n",
      " [18116.666]\n",
      " [18116.617]]\n",
      "1 cost: 7.843981e+16 \n",
      "Prediction: [[-2.8007066e+08]\n",
      " [-2.8007709e+08]\n",
      " [-2.8008067e+08]\n",
      " [-2.8008474e+08]\n",
      " [-2.8008166e+08]\n",
      " [-2.8008243e+08]\n",
      " [-2.8009050e+08]\n",
      " [-2.8009213e+08]]\n",
      "2 cost: 7.0729993e+24 \n",
      "Prediction: [[2.6593987e+12]\n",
      " [2.6594598e+12]\n",
      " [2.6594939e+12]\n",
      " [2.6595324e+12]\n",
      " [2.6595033e+12]\n",
      " [2.6595109e+12]\n",
      " [2.6595875e+12]\n",
      " [2.6596029e+12]]\n",
      "3 cost: 6.3777967e+32 \n",
      "Prediction: [[-2.5253233e+16]\n",
      " [-2.5253815e+16]\n",
      " [-2.5254137e+16]\n",
      " [-2.5254502e+16]\n",
      " [-2.5254225e+16]\n",
      " [-2.5254296e+16]\n",
      " [-2.5255022e+16]\n",
      " [-2.5255170e+16]]\n",
      "4 cost: inf \n",
      "Prediction: [[2.3980074e+20]\n",
      " [2.3980625e+20]\n",
      " [2.3980933e+20]\n",
      " [2.3981277e+20]\n",
      " [2.3981017e+20]\n",
      " [2.3981084e+20]\n",
      " [2.3981775e+20]\n",
      " [2.3981914e+20]]\n",
      "5 cost: inf \n",
      "Prediction: [[-2.2771102e+24]\n",
      " [-2.2771626e+24]\n",
      " [-2.2771919e+24]\n",
      " [-2.2772248e+24]\n",
      " [-2.2771998e+24]\n",
      " [-2.2772063e+24]\n",
      " [-2.2772717e+24]\n",
      " [-2.2772849e+24]]\n",
      "6 cost: inf \n",
      "Prediction: [[2.1623081e+28]\n",
      " [2.1623579e+28]\n",
      " [2.1623858e+28]\n",
      " [2.1624169e+28]\n",
      " [2.1623933e+28]\n",
      " [2.1623995e+28]\n",
      " [2.1624616e+28]\n",
      " [2.1624741e+28]]\n",
      "7 cost: inf \n",
      "Prediction: [[-2.0532940e+32]\n",
      " [-2.0533412e+32]\n",
      " [-2.0533677e+32]\n",
      " [-2.0533971e+32]\n",
      " [-2.0533746e+32]\n",
      " [-2.0533806e+32]\n",
      " [-2.0534396e+32]\n",
      " [-2.0534516e+32]]\n",
      "8 cost: inf \n",
      "Prediction: [[1.9497758e+36]\n",
      " [1.9498206e+36]\n",
      " [1.9498456e+36]\n",
      " [1.9498737e+36]\n",
      " [1.9498526e+36]\n",
      " [1.9498582e+36]\n",
      " [1.9499141e+36]\n",
      " [1.9499253e+36]]\n",
      "9 cost: inf \n",
      "Prediction: [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "10 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 cost: nan \n",
      "Prediction: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(101):\n",
    "        cv,hv,_=sess.run([cost, hf, train], feed_dict={x:xdata,y:ydata})\n",
    "        print(step,\"cost:\",cv,\"\\nPrediction:\",hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xor 문제를 텐서플로우로 구현\n",
    "# 단일, 멀티 퍼셉트론으로 각각 구현\n",
    "xdata=np.array([[0,0],\n",
    "         [0,1],\n",
    "         [1,0],\n",
    "         [1,1]])\n",
    "ydata=np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# 트레이닝 횟수 : 10000번, lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,2])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([2,1]))\n",
    "b=tf.Variable(tf.random_normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.sigmoid(tf.matmul(x,w)+b)\n",
    "cost=-tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2581222\n",
      "500 0.6931862\n",
      "1000 0.69314796\n",
      "1500 0.69314724\n",
      "2000 0.69314724\n",
      "2500 0.6931472\n",
      "3000 0.6931472\n",
      "3500 0.6931472\n",
      "4000 0.6931472\n",
      "4500 0.6931472\n",
      "5000 0.6931472\n",
      "5500 0.6931472\n",
      "6000 0.6931472\n",
      "6500 0.6931472\n",
      "7000 0.6931472\n",
      "7500 0.6931472\n",
      "8000 0.6931472\n",
      "8500 0.6931472\n",
      "9000 0.6931472\n",
      "9500 0.6931472\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        _, cv, wv = sess.run(\n",
    "                  [train, cost, W], feed_dict={x: xdata, y: ydata}\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(step, cv, wv)\n",
    "\n",
    "    h, c, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티레이어 퍼셉트론 기반 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7699648\n",
      "100 0.69296104\n",
      "200 0.6929489\n",
      "300 0.6929409\n",
      "400 0.6929326\n",
      "500 0.6929242\n",
      "600 0.6929155\n",
      "700 0.69290656\n",
      "800 0.6928974\n",
      "900 0.692888\n",
      "1000 0.6928783\n",
      "1100 0.69286835\n",
      "1200 0.69285804\n",
      "1300 0.6928475\n",
      "1400 0.6928365\n",
      "1500 0.6928252\n",
      "1600 0.6928136\n",
      "1700 0.6928014\n",
      "1800 0.6927889\n",
      "1900 0.69277596\n",
      "2000 0.6927626\n",
      "2100 0.6927488\n",
      "2200 0.69273436\n",
      "2300 0.69271934\n",
      "2400 0.69270384\n",
      "2500 0.69268763\n",
      "2600 0.6926709\n",
      "2700 0.6926534\n",
      "2800 0.6926353\n",
      "2900 0.69261634\n",
      "3000 0.69259655\n",
      "3100 0.692576\n",
      "3200 0.6925544\n",
      "3300 0.692532\n",
      "3400 0.69250846\n",
      "3500 0.6924839\n",
      "3600 0.69245815\n",
      "3700 0.6924312\n",
      "3800 0.692403\n",
      "3900 0.69237334\n",
      "4000 0.69234216\n",
      "4100 0.6923095\n",
      "4200 0.69227505\n",
      "4300 0.69223887\n",
      "4400 0.6922008\n",
      "4500 0.69216055\n",
      "4600 0.69211817\n",
      "4700 0.69207346\n",
      "4800 0.69202596\n",
      "4900 0.69197595\n",
      "5000 0.6919229\n",
      "5100 0.69186664\n",
      "5200 0.691807\n",
      "5300 0.6917436\n",
      "5400 0.69167614\n",
      "5500 0.69160426\n",
      "5600 0.6915277\n",
      "5700 0.69144607\n",
      "5800 0.6913588\n",
      "5900 0.6912653\n",
      "6000 0.6911652\n",
      "6100 0.6910577\n",
      "6200 0.69094217\n",
      "6300 0.69081765\n",
      "6400 0.69068354\n",
      "6500 0.6905387\n",
      "6600 0.690382\n",
      "6700 0.690212\n",
      "6800 0.6900274\n",
      "6900 0.68982637\n",
      "7000 0.68960714\n",
      "7100 0.6893676\n",
      "7200 0.68910503\n",
      "7300 0.6888166\n",
      "7400 0.68849903\n",
      "7500 0.6881484\n",
      "7600 0.6877601\n",
      "7700 0.687329\n",
      "7800 0.68684894\n",
      "7900 0.68631256\n",
      "8000 0.6857115\n",
      "8100 0.68503565\n",
      "8200 0.6842731\n",
      "8300 0.6834098\n",
      "8400 0.68242913\n",
      "8500 0.68131125\n",
      "8600 0.6800326\n",
      "8700 0.67856544\n",
      "8800 0.676877\n",
      "8900 0.67492896\n",
      "9000 0.6726769\n",
      "9100 0.67007023\n",
      "9200 0.66705203\n",
      "9300 0.6635611\n",
      "9400 0.6595343\n",
      "9500 0.65491056\n",
      "9600 0.6496385\n",
      "9700 0.6436845\n",
      "9800 0.6370433\n",
      "9900 0.62974846\n",
      "10000 0.6218785\n",
      "\n",
      "Hypothesis:  [[0.59916764]\n",
      " [0.5525457 ]\n",
      " [0.55646354]\n",
      " [0.32537898]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,2])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "# 히든레이어 1\n",
    "w1=tf.Variable(tf.random_normal([2,2]))\n",
    "b1=tf.Variable(tf.random_normal([2]))\n",
    "layer1=tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "# 히든레이어 2\n",
    "w2=tf.Variable(tf.random_normal([2,2]))\n",
    "b2=tf.Variable(tf.random_normal([2]))\n",
    "layer2=tf.sigmoid(tf.matmul(layer1,w2)+b2)\n",
    "\n",
    "# 히든레이어 2\n",
    "w3=tf.Variable(tf.random_normal([2,1]))\n",
    "b3=tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.sigmoid(tf.matmul(layer2,w3)+b3)\n",
    "\n",
    "cost=-tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "\n",
    "predicted=tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype=tf.float32))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        _, cv, = sess.run(\n",
    "                  [train, cost], feed_dict={x: xdata, y: ydata}\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(step, cv)\n",
    "\n",
    "    h, c, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.366342\n",
      "100 0.0639506\n",
      "200 0.021660903\n",
      "300 0.011551621\n",
      "400 0.0075021717\n",
      "500 0.0053923046\n",
      "600 0.0041418285\n",
      "700 0.0033248079\n",
      "800 0.002759356\n",
      "900 0.002337787\n",
      "1000 0.0020295659\n",
      "1100 0.0017842192\n",
      "1200 0.0015851102\n",
      "1300 0.0009643694\n",
      "1400 0.00067793246\n",
      "1500 0.00051825785\n",
      "1600 0.00041986385\n",
      "1700 0.00035275868\n",
      "1800 0.00030393375\n",
      "1900 0.00026677543\n",
      "2000 0.00023756747\n",
      "2100 0.00021398213\n",
      "2200 0.00019452757\n",
      "2300 0.00017823417\n",
      "2400 0.00016429662\n",
      "2500 0.00015238671\n",
      "2600 0.00014195278\n",
      "2700 0.00013281585\n",
      "2800 0.00012475226\n",
      "2900 0.000117597985\n",
      "3000 0.00011112944\n",
      "3100 0.000105346575\n",
      "3200 0.000100100326\n",
      "3300 9.5301264e-05\n",
      "3400 9.099404e-05\n",
      "3500 8.695515e-05\n",
      "3600 8.327397e-05\n",
      "3700 7.989089e-05\n",
      "3800 7.679099e-05\n",
      "3900 7.384015e-05\n",
      "4000 7.117248e-05\n",
      "4100 6.862405e-05\n",
      "4200 6.625447e-05\n",
      "4300 6.407864e-05\n",
      "4400 6.200714e-05\n",
      "4500 6.0054863e-05\n",
      "4600 5.8236725e-05\n",
      "4700 5.6478202e-05\n",
      "4800 5.4853816e-05\n",
      "4900 5.328904e-05\n",
      "5000 5.1813684e-05\n",
      "5100 5.042775e-05\n",
      "5200 4.9086524e-05\n",
      "5300 4.784962e-05\n",
      "5400 4.6657424e-05\n",
      "5500 4.549504e-05\n",
      "5600 4.4407167e-05\n",
      "5700 4.3364005e-05\n",
      "5800 4.2380452e-05\n",
      "5900 4.1441606e-05\n",
      "6000 4.051767e-05\n",
      "6100 3.963844e-05\n",
      "6200 3.8803915e-05\n",
      "6300 3.7969396e-05\n",
      "6400 3.7194484e-05\n",
      "6500 3.6434474e-05\n",
      "6600 3.574898e-05\n",
      "6700 3.5018777e-05\n",
      "6800 3.4363085e-05\n",
      "6900 3.37223e-05\n",
      "7000 3.309641e-05\n",
      "7100 3.2515232e-05\n",
      "7200 3.1948955e-05\n",
      "7300 3.135288e-05\n",
      "7400 3.0816405e-05\n",
      "7500 3.030974e-05\n",
      "7600 2.9773271e-05\n",
      "7700 2.9266605e-05\n",
      "7800 2.8819548e-05\n",
      "7900 2.8357588e-05\n",
      "8000 2.7925435e-05\n",
      "8100 2.7463477e-05\n",
      "8200 2.7061127e-05\n",
      "8300 2.6658778e-05\n",
      "8400 2.6226622e-05\n",
      "8500 2.5839176e-05\n",
      "8600 2.546663e-05\n",
      "8700 2.5094085e-05\n",
      "8800 2.4736442e-05\n",
      "8900 2.4393701e-05\n",
      "9000 2.4065861e-05\n",
      "9100 2.3723122e-05\n",
      "9200 2.3410183e-05\n",
      "9300 2.3082344e-05\n",
      "9400 2.2814113e-05\n",
      "9500 2.2516077e-05\n",
      "9600 2.2203141e-05\n",
      "9700 2.1905109e-05\n",
      "9800 2.166668e-05\n",
      "9900 2.1368645e-05\n",
      "10000 2.1115316e-05\n",
      "\n",
      "Hypothesis:  [[5.0540890e-05]\n",
      " [9.9998522e-01]\n",
      " [9.9999344e-01]\n",
      " [1.2552553e-05]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# widw & deep\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=[None,2])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "# 히든레이어 1\n",
    "w1=tf.Variable(tf.random_normal([2,10]))\n",
    "b1=tf.Variable(tf.random_normal([10]))\n",
    "layer1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "# 히든레이어 2\n",
    "w2=tf.Variable(tf.random_normal([10,10]))\n",
    "b2=tf.Variable(tf.random_normal([10]))\n",
    "layer2=tf.nn.relu(tf.matmul(layer1,w2)+b2)\n",
    "\n",
    "# 히든레이어 3\n",
    "w3=tf.Variable(tf.random_normal([10,10]))\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "layer3=tf.nn.relu(tf.matmul(layer2,w3)+b3)\n",
    "\n",
    "# 히든레이어 4\n",
    "w4=tf.Variable(tf.random_normal([10,1]))\n",
    "b4=tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.sigmoid(tf.matmul(layer3,w4)+b4)\n",
    "\n",
    "\n",
    "cost=-tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "\n",
    "predicted=tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype=tf.float32))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        _, cv, = sess.run(\n",
    "                  [train, cost], feed_dict={x: xdata, y: ydata}\n",
    "        )\n",
    "        if step % 100 == 0:\n",
    "            print(step, cv)\n",
    "\n",
    "    h, c, a = sess.run([hf, predicted, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. mnist - 90% 정확도 넘게 (텐서플로우)\n",
    "# 2. pima-indians-diabetes 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True) # MNIST_data 위치에 저장, one_hot 되어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10]) # mnist에서 0~9까지 총 10가지 숫자가 나올수 있기 때문에 10으로 줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(tf.random_normal([784,256]))\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "layer1=tf.sigmoid(tf.matmul(x,w1)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=tf.Variable(tf.random_normal([256,10]))\n",
    "b2=tf.Variable(tf.random_normal([10]))\n",
    "hf=tf.nn.softmax(tf.matmul(layer1,w2)+b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCorrect=tf.equal(tf.argmax(hf,1), tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(isCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs=300\n",
    "batchSize=100\n",
    "numIter=int(mnist.train.num_examples/batchSize)\n",
    "# 60000 / 100 = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에폭:0001, cost:2.183302653\n",
      "에폭:0002, cost:0.935786237\n",
      "에폭:0003, cost:0.730903059\n",
      "에폭:0004, cost:0.611331568\n",
      "에폭:0005, cost:0.543310336\n",
      "에폭:0006, cost:0.487690378\n",
      "에폭:0007, cost:0.445709616\n",
      "에폭:0008, cost:0.412744742\n",
      "에폭:0009, cost:0.385135649\n",
      "에폭:0010, cost:0.360606949\n",
      "에폭:0011, cost:0.344021644\n",
      "에폭:0012, cost:0.325138643\n",
      "에폭:0013, cost:0.309234860\n",
      "에폭:0014, cost:0.294522350\n",
      "에폭:0015, cost:0.280913663\n",
      "에폭:0016, cost:0.271099304\n",
      "에폭:0017, cost:0.263518946\n",
      "에폭:0018, cost:0.252355490\n",
      "에폭:0019, cost:0.240534138\n",
      "에폭:0020, cost:0.236922502\n",
      "에폭:0021, cost:0.228423298\n",
      "에폭:0022, cost:0.220067570\n",
      "에폭:0023, cost:0.214020308\n",
      "에폭:0024, cost:0.209192847\n",
      "에폭:0025, cost:0.202688233\n",
      "에폭:0026, cost:0.198053456\n",
      "에폭:0027, cost:0.191474659\n",
      "에폭:0028, cost:0.189558957\n",
      "에폭:0029, cost:0.184023939\n",
      "에폭:0030, cost:0.177941162\n",
      "에폭:0031, cost:0.177631528\n",
      "에폭:0032, cost:0.170314520\n",
      "에폭:0033, cost:0.168975745\n",
      "에폭:0034, cost:0.163645932\n",
      "에폭:0035, cost:0.163530560\n",
      "에폭:0036, cost:0.155657331\n",
      "에폭:0037, cost:0.155652508\n",
      "에폭:0038, cost:0.152442977\n",
      "에폭:0039, cost:0.147820627\n",
      "에폭:0040, cost:0.148757927\n",
      "에폭:0041, cost:0.141649519\n",
      "에폭:0042, cost:0.140464046\n",
      "에폭:0043, cost:0.140509535\n",
      "에폭:0044, cost:0.134432262\n",
      "에폭:0045, cost:0.135410630\n",
      "에폭:0046, cost:0.132762848\n",
      "에폭:0047, cost:0.128364029\n",
      "에폭:0048, cost:0.127330013\n",
      "에폭:0049, cost:0.126566558\n",
      "에폭:0050, cost:0.124568001\n",
      "에폭:0051, cost:0.120446636\n",
      "에폭:0052, cost:0.119623597\n",
      "에폭:0053, cost:0.120012924\n",
      "에폭:0054, cost:0.116817812\n",
      "에폭:0055, cost:0.113460350\n",
      "에폭:0056, cost:0.113026252\n",
      "에폭:0057, cost:0.111489796\n",
      "에폭:0058, cost:0.109836419\n",
      "에폭:0059, cost:0.108186454\n",
      "에폭:0060, cost:0.106845671\n",
      "에폭:0061, cost:0.105821783\n",
      "에폭:0062, cost:0.103069390\n",
      "에폭:0063, cost:0.103267907\n",
      "에폭:0064, cost:0.100220824\n",
      "에폭:0065, cost:0.099799575\n",
      "에폭:0066, cost:0.099255133\n",
      "에폭:0067, cost:0.097250949\n",
      "에폭:0068, cost:0.096010242\n",
      "에폭:0069, cost:0.093861519\n",
      "에폭:0070, cost:0.094069480\n",
      "에폭:0071, cost:0.092512135\n",
      "에폭:0072, cost:0.090265782\n",
      "에폭:0073, cost:0.090310279\n",
      "에폭:0074, cost:0.090308179\n",
      "에폭:0075, cost:0.087807176\n",
      "에폭:0076, cost:0.085701544\n",
      "에폭:0077, cost:0.086082920\n",
      "에폭:0078, cost:0.085601625\n",
      "에폭:0079, cost:0.083865011\n",
      "에폭:0080, cost:0.083488594\n",
      "에폭:0081, cost:0.081627198\n",
      "에폭:0082, cost:0.080531622\n",
      "에폭:0083, cost:0.080129581\n",
      "에폭:0084, cost:0.078717047\n",
      "에폭:0085, cost:0.078505563\n",
      "에폭:0086, cost:0.076310706\n",
      "에폭:0087, cost:0.076885186\n",
      "에폭:0088, cost:0.076095133\n",
      "에폭:0089, cost:0.074206640\n",
      "에폭:0090, cost:0.074857004\n",
      "에폭:0091, cost:0.072958131\n",
      "에폭:0092, cost:0.072231343\n",
      "에폭:0093, cost:0.071005119\n",
      "에폭:0094, cost:0.071092267\n",
      "에폭:0095, cost:0.069815153\n",
      "에폭:0096, cost:0.069565876\n",
      "에폭:0097, cost:0.068715383\n",
      "에폭:0098, cost:0.066976233\n",
      "에폭:0099, cost:0.067241360\n",
      "에폭:0100, cost:0.066003719\n",
      "에폭:0101, cost:0.065958125\n",
      "에폭:0102, cost:0.064476192\n",
      "에폭:0103, cost:0.064661368\n",
      "에폭:0104, cost:0.064049440\n",
      "에폭:0105, cost:0.062294356\n",
      "에폭:0106, cost:0.062886750\n",
      "에폭:0107, cost:0.060844586\n",
      "에폭:0108, cost:0.061426358\n",
      "에폭:0109, cost:0.060787981\n",
      "에폭:0110, cost:0.058761649\n",
      "에폭:0111, cost:0.059463875\n",
      "에폭:0112, cost:0.058937915\n",
      "에폭:0113, cost:0.057799700\n",
      "에폭:0114, cost:0.057494149\n",
      "에폭:0115, cost:0.056066893\n",
      "에폭:0116, cost:0.056701404\n",
      "에폭:0117, cost:0.055245896\n",
      "에폭:0118, cost:0.055682721\n",
      "에폭:0119, cost:0.055077263\n",
      "에폭:0120, cost:0.053566879\n",
      "에폭:0121, cost:0.053164188\n",
      "에폭:0122, cost:0.052897850\n",
      "에폭:0123, cost:0.053056631\n",
      "에폭:0124, cost:0.051723794\n",
      "에폭:0125, cost:0.051421297\n",
      "에폭:0126, cost:0.051634240\n",
      "에폭:0127, cost:0.049609724\n",
      "에폭:0128, cost:0.050578260\n",
      "에폭:0129, cost:0.049022158\n",
      "에폭:0130, cost:0.049114786\n",
      "에폭:0131, cost:0.048498721\n",
      "에폭:0132, cost:0.047668056\n",
      "에폭:0133, cost:0.048277405\n",
      "에폭:0134, cost:0.047075082\n",
      "에폭:0135, cost:0.045963472\n",
      "에폭:0136, cost:0.047365136\n",
      "에폭:0137, cost:0.045574474\n",
      "에폭:0138, cost:0.045195224\n",
      "에폭:0139, cost:0.044941020\n",
      "에폭:0140, cost:0.044213981\n",
      "에폭:0141, cost:0.044179810\n",
      "에폭:0142, cost:0.044103784\n",
      "에폭:0143, cost:0.042235968\n",
      "에폭:0144, cost:0.043583948\n",
      "에폭:0145, cost:0.042884636\n",
      "에폭:0146, cost:0.042174823\n",
      "에폭:0147, cost:0.041518222\n",
      "에폭:0148, cost:0.041362744\n",
      "에폭:0149, cost:0.041042673\n",
      "에폭:0150, cost:0.041144630\n",
      "에폭:0151, cost:0.039745711\n",
      "에폭:0152, cost:0.040218899\n",
      "에폭:0153, cost:0.039162641\n",
      "에폭:0154, cost:0.039440911\n",
      "에폭:0155, cost:0.038751191\n",
      "에폭:0156, cost:0.038496104\n",
      "에폭:0157, cost:0.037647775\n",
      "에폭:0158, cost:0.038123305\n",
      "에폭:0159, cost:0.037474562\n",
      "에폭:0160, cost:0.036717330\n",
      "에폭:0161, cost:0.036853074\n",
      "에폭:0162, cost:0.036335356\n",
      "에폭:0163, cost:0.035480127\n",
      "에폭:0164, cost:0.035507938\n",
      "에폭:0165, cost:0.036073987\n",
      "에폭:0166, cost:0.034779577\n",
      "에폭:0167, cost:0.035177509\n",
      "에폭:0168, cost:0.034906525\n",
      "에폭:0169, cost:0.034072554\n",
      "에폭:0170, cost:0.034142734\n",
      "에폭:0171, cost:0.033199573\n",
      "에폭:0172, cost:0.033218401\n",
      "에폭:0173, cost:0.033250859\n",
      "에폭:0174, cost:0.032532771\n",
      "에폭:0175, cost:0.032583874\n",
      "에폭:0176, cost:0.031941662\n",
      "에폭:0177, cost:0.032107718\n",
      "에폭:0178, cost:0.031057230\n",
      "에폭:0179, cost:0.032021958\n",
      "에폭:0180, cost:0.030773053\n",
      "에폭:0181, cost:0.031119987\n",
      "에폭:0182, cost:0.030809573\n",
      "에폭:0183, cost:0.030054105\n",
      "에폭:0184, cost:0.030143152\n",
      "에폭:0185, cost:0.029805500\n",
      "에폭:0186, cost:0.029427946\n",
      "에폭:0187, cost:0.029177096\n",
      "에폭:0188, cost:0.029496069\n",
      "에폭:0189, cost:0.028847514\n",
      "에폭:0190, cost:0.028399789\n",
      "에폭:0191, cost:0.028602410\n",
      "에폭:0192, cost:0.028038421\n",
      "에폭:0193, cost:0.027572059\n",
      "에폭:0194, cost:0.027964361\n",
      "에폭:0195, cost:0.026905866\n",
      "에폭:0196, cost:0.027668068\n",
      "에폭:0197, cost:0.026819459\n",
      "에폭:0198, cost:0.026367042\n",
      "에폭:0199, cost:0.026918035\n",
      "에폭:0200, cost:0.026409780\n",
      "에폭:0201, cost:0.025858689\n",
      "에폭:0202, cost:0.026063349\n",
      "에폭:0203, cost:0.025818881\n",
      "에폭:0204, cost:0.025171013\n",
      "에폭:0205, cost:0.025477989\n",
      "에폭:0206, cost:0.025003630\n",
      "에폭:0207, cost:0.025014816\n",
      "에폭:0208, cost:0.024525002\n",
      "에폭:0209, cost:0.024576684\n",
      "에폭:0210, cost:0.024286789\n",
      "에폭:0211, cost:0.023721830\n",
      "에폭:0212, cost:0.023997970\n",
      "에폭:0213, cost:0.023747600\n",
      "에폭:0214, cost:0.023922501\n",
      "에폭:0215, cost:0.023260922\n",
      "에폭:0216, cost:0.023239358\n",
      "에폭:0217, cost:0.022893172\n",
      "에폭:0218, cost:0.023086788\n",
      "에폭:0219, cost:0.022355674\n",
      "에폭:0220, cost:0.022580428\n",
      "에폭:0221, cost:0.022195511\n",
      "에폭:0222, cost:0.022094565\n",
      "에폭:0223, cost:0.021813598\n",
      "에폭:0224, cost:0.021696687\n",
      "에폭:0225, cost:0.021866655\n",
      "에폭:0226, cost:0.021426202\n",
      "에폭:0227, cost:0.021197164\n",
      "에폭:0228, cost:0.020976738\n",
      "에폭:0229, cost:0.021071823\n",
      "에폭:0230, cost:0.020748601\n",
      "에폭:0231, cost:0.020827732\n",
      "에폭:0232, cost:0.020516097\n",
      "에폭:0233, cost:0.020321464\n",
      "에폭:0234, cost:0.020134615\n",
      "에폭:0235, cost:0.020005670\n",
      "에폭:0236, cost:0.019763521\n",
      "에폭:0237, cost:0.019981254\n",
      "에폭:0238, cost:0.019487698\n",
      "에폭:0239, cost:0.019657919\n",
      "에폭:0240, cost:0.019001128\n",
      "에폭:0241, cost:0.019572946\n",
      "에폭:0242, cost:0.018959116\n",
      "에폭:0243, cost:0.019115859\n",
      "에폭:0244, cost:0.018650496\n",
      "에폭:0245, cost:0.018767241\n",
      "에폭:0246, cost:0.018506694\n",
      "에폭:0247, cost:0.018187699\n",
      "에폭:0248, cost:0.018265113\n",
      "에폭:0249, cost:0.017999597\n",
      "에폭:0250, cost:0.018142030\n",
      "에폭:0251, cost:0.017750485\n",
      "에폭:0252, cost:0.017789763\n",
      "에폭:0253, cost:0.017668546\n",
      "에폭:0254, cost:0.017204414\n",
      "에폭:0255, cost:0.017571326\n",
      "에폭:0256, cost:0.017283456\n",
      "에폭:0257, cost:0.017221047\n",
      "에폭:0258, cost:0.017110066\n",
      "에폭:0259, cost:0.016791534\n",
      "에폭:0260, cost:0.016904435\n",
      "에폭:0261, cost:0.016453208\n",
      "에폭:0262, cost:0.016461963\n",
      "에폭:0263, cost:0.016584925\n",
      "에폭:0264, cost:0.016145845\n",
      "에폭:0265, cost:0.016125418\n",
      "에폭:0266, cost:0.016300323\n",
      "에폭:0267, cost:0.015666018\n",
      "에폭:0268, cost:0.016239298\n",
      "에폭:0269, cost:0.015805202\n",
      "에폭:0270, cost:0.015561559\n",
      "에폭:0271, cost:0.015511004\n",
      "에폭:0272, cost:0.015802054\n",
      "에폭:0273, cost:0.015124298\n",
      "에폭:0274, cost:0.015485560\n",
      "에폭:0275, cost:0.014935607\n",
      "에폭:0276, cost:0.015152709\n",
      "에폭:0277, cost:0.014874217\n",
      "에폭:0278, cost:0.014816826\n",
      "에폭:0279, cost:0.014740232\n",
      "에폭:0280, cost:0.014740417\n",
      "에폭:0281, cost:0.014729933\n",
      "에폭:0282, cost:0.014288323\n",
      "에폭:0283, cost:0.014386414\n",
      "에폭:0284, cost:0.014167706\n",
      "에폭:0285, cost:0.014380023\n",
      "에폭:0286, cost:0.014127848\n",
      "에폭:0287, cost:0.013712529\n",
      "에폭:0288, cost:0.014107244\n",
      "에폭:0289, cost:0.013973997\n",
      "에폭:0290, cost:0.013696959\n",
      "에폭:0291, cost:0.013735946\n",
      "에폭:0292, cost:0.013482320\n",
      "에폭:0293, cost:0.013656801\n",
      "에폭:0294, cost:0.013173419\n",
      "에폭:0295, cost:0.013370077\n",
      "에폭:0296, cost:0.013166844\n",
      "에폭:0297, cost:0.013300315\n",
      "에폭:0298, cost:0.012915085\n",
      "에폭:0299, cost:0.013115186\n",
      "에폭:0300, cost:0.012980217\n",
      "정확도: 0.9525\n",
      "레이블: [7]\n",
      "예측: [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 트레이닝\n",
    "    for epoch in range(numEpochs): # 15 epochs\n",
    "        avgCv=0\n",
    "        for i in range(numIter): # 600\n",
    "            batchX, batchY=mnist.train.next_batch(batchSize) # 한번에 몇개의 data를 읽어올지\n",
    "            _,cv=sess.run([train, cost],feed_dict={x:batchX, y:batchY})\n",
    "            avgCv+=cv/numIter\n",
    "        print(\"에폭:{:04d}, cost:{:.9f}\".format(epoch+1, avgCv))  #{04d} 4자리 십진수\n",
    "    print(\"정확도:\",accuracy.eval(session=sess, feed_dict={x:mnist.test.images,y:mnist.test.labels}))\n",
    "    \n",
    "    r=random.randint(0,mnist.test.num_examples-1)\n",
    "    print(\"레이블:\",sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "    print(\"예측:\",sess.run(tf.argmax(hf,1),feed_dict={x:mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap=\"Greys\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"C:/Users/student/Downloads/데이터들/dataset/pima-indians-diabetes.csv\",header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "y_data=data.iloc[:,[8]]\n",
    "xdata=data.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape # 768, 1\n",
    "xdata.shape # 768, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xdata)\n",
    "x_scaled = scaler.transform(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data,y_data,test_size=0.3, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,8])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(tf.random_normal([8,8]))\n",
    "b1=tf.Variable(tf.random_normal([8]))\n",
    "layer1=tf.sigmoid(tf.matmul(x,w1)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=tf.Variable(tf.random_normal([8,8]))\n",
    "b2=tf.Variable(tf.random_normal([8]))\n",
    "layer2=tf.sigmoid(tf.matmul(layer1,w2)+b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3=tf.Variable(tf.random_normal([8,1]))\n",
    "b3=tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.sigmoid(tf.matmul(layer2,w3)+b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=-tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=tf.cast(hf>0.5, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8860155\n",
      "500 0.66135806\n",
      "1000 0.62588125\n",
      "1500 0.6029617\n",
      "2000 0.5847513\n",
      "2500 0.5686955\n",
      "3000 0.5539643\n",
      "3500 0.5403729\n",
      "4000 0.52795666\n",
      "4500 0.5167828\n",
      "5000 0.50687504\n",
      "5500 0.4981986\n",
      "6000 0.49067235\n",
      "6500 0.48418838\n",
      "7000 0.4786276\n",
      "7500 0.47387013\n",
      "8000 0.46980157\n",
      "8500 0.46631593\n",
      "9000 0.4633179\n",
      "9500 0.46072295\n",
      "10000 0.45845807\n",
      "10500 0.45646068\n",
      "11000 0.45467854\n",
      "11500 0.4530686\n",
      "12000 0.45159605\n",
      "12500 0.45023283\n",
      "13000 0.44895718\n",
      "13500 0.44775173\n",
      "14000 0.4466031\n",
      "14500 0.4455007\n",
      "15000 0.44443622\n",
      "15500 0.44340312\n",
      "16000 0.44239607\n",
      "16500 0.44141108\n",
      "17000 0.44044444\n",
      "17500 0.43949372\n",
      "18000 0.43855622\n",
      "18500 0.43763044\n",
      "19000 0.4367148\n",
      "19500 0.43580806\n",
      "20000 0.43490976\n",
      "20500 0.4340192\n",
      "21000 0.43313617\n",
      "21500 0.43226093\n",
      "22000 0.4313937\n",
      "22500 0.43053502\n",
      "23000 0.4296856\n",
      "23500 0.4288462\n",
      "24000 0.42801794\n",
      "24500 0.42720136\n",
      "25000 0.42639717\n",
      "25500 0.42560628\n",
      "26000 0.42482904\n",
      "26500 0.42406562\n",
      "27000 0.42331612\n",
      "27500 0.4225807\n",
      "28000 0.4218591\n",
      "28500 0.4211512\n",
      "29000 0.42045638\n",
      "29500 0.41977412\n",
      "30000 0.419104\n",
      "30500 0.41844565\n",
      "31000 0.41779828\n",
      "31500 0.4171616\n",
      "32000 0.41653496\n",
      "32500 0.41591835\n",
      "33000 0.41531098\n",
      "33500 0.4147128\n",
      "34000 0.41412362\n",
      "34500 0.41354337\n",
      "35000 0.41297176\n",
      "35500 0.41240865\n",
      "36000 0.4118541\n",
      "36500 0.41130808\n",
      "37000 0.4107702\n",
      "37500 0.41024044\n",
      "38000 0.40971872\n",
      "38500 0.40920505\n",
      "39000 0.4086991\n",
      "39500 0.408201\n",
      "40000 0.40771088\n",
      "40500 0.40722814\n",
      "41000 0.40675297\n",
      "41500 0.40628532\n",
      "42000 0.40582508\n",
      "42500 0.4053723\n",
      "43000 0.4049272\n",
      "43500 0.40448955\n",
      "44000 0.4040593\n",
      "44500 0.4036365\n",
      "45000 0.4032212\n",
      "45500 0.40281364\n",
      "46000 0.4024134\n",
      "46500 0.40202048\n",
      "47000 0.4016348\n",
      "47500 0.40125653\n",
      "48000 0.40088522\n",
      "48500 0.40052083\n",
      "49000 0.40016332\n",
      "49500 0.39981246\n",
      "50000 0.39946812\n",
      "50500 0.39913002\n",
      "51000 0.39879808\n",
      "51500 0.3984721\n",
      "52000 0.3981519\n",
      "52500 0.39783704\n",
      "53000 0.39752758\n",
      "53500 0.3972235\n",
      "54000 0.39692432\n",
      "54500 0.39662996\n",
      "55000 0.3963403\n",
      "55500 0.39605534\n",
      "56000 0.3957746\n",
      "56500 0.39549816\n",
      "57000 0.39522567\n",
      "57500 0.3949573\n",
      "58000 0.39469278\n",
      "58500 0.3944319\n",
      "59000 0.39417458\n",
      "59500 0.3939208\n",
      "60000 0.3936705\n",
      "60500 0.3934236\n",
      "61000 0.3931796\n",
      "61500 0.39293873\n",
      "62000 0.3927009\n",
      "62500 0.39246607\n",
      "63000 0.39223403\n",
      "63500 0.39200473\n",
      "64000 0.3917781\n",
      "64500 0.3915541\n",
      "65000 0.39133263\n",
      "65500 0.3911138\n",
      "66000 0.3908974\n",
      "66500 0.39068335\n",
      "67000 0.3904715\n",
      "67500 0.39026192\n",
      "68000 0.39005476\n",
      "68500 0.3898496\n",
      "69000 0.3896465\n",
      "69500 0.38944557\n",
      "70000 0.38924655\n",
      "70500 0.38904938\n",
      "71000 0.38885418\n",
      "71500 0.3886608\n",
      "72000 0.38846925\n",
      "72500 0.38827944\n",
      "73000 0.38809127\n",
      "73500 0.38790482\n",
      "74000 0.3877199\n",
      "74500 0.38753644\n",
      "75000 0.38735458\n",
      "75500 0.38717437\n",
      "76000 0.38699546\n",
      "76500 0.38681778\n",
      "77000 0.38664144\n",
      "77500 0.38646638\n",
      "78000 0.38629252\n",
      "78500 0.3861197\n",
      "79000 0.38594812\n",
      "79500 0.38577747\n",
      "80000 0.38560787\n",
      "80500 0.38543937\n",
      "81000 0.38527155\n",
      "81500 0.3851047\n",
      "82000 0.38493866\n",
      "82500 0.38477316\n",
      "83000 0.38460818\n",
      "83500 0.38444388\n",
      "84000 0.38428026\n",
      "84500 0.3841171\n",
      "85000 0.38395438\n",
      "85500 0.3837919\n",
      "86000 0.38362986\n",
      "86500 0.383468\n",
      "87000 0.38330624\n",
      "87500 0.3831445\n",
      "88000 0.38298276\n",
      "88500 0.38282105\n",
      "89000 0.38265923\n",
      "89500 0.3824974\n",
      "90000 0.38233522\n",
      "90500 0.38217282\n",
      "91000 0.38200992\n",
      "91500 0.3818465\n",
      "92000 0.38168254\n",
      "92500 0.38151792\n",
      "93000 0.3813524\n",
      "93500 0.3811862\n",
      "94000 0.381019\n",
      "94500 0.3808508\n",
      "95000 0.38068148\n",
      "95500 0.38051102\n",
      "96000 0.38033947\n",
      "96500 0.38016644\n",
      "97000 0.379992\n",
      "97500 0.37981597\n",
      "98000 0.3796383\n",
      "98500 0.3794586\n",
      "99000 0.3792772\n",
      "99500 0.37909362\n",
      "100000 0.378908\n",
      "정확도 :  0.7835498\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100001):\n",
    "        cv, _=sess.run([cost, train], feed_dict={x:x_train, y:y_train})\n",
    "        \n",
    "        if step%500==0:\n",
    "            print(step, cv)\n",
    "    _,pv,av=sess.run([hf, predicted, accuracy], feed_dict={x:x_test, y:y_test})\n",
    "    print(\"정확도 : \",av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
