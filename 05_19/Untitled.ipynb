{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/student/Downloads/데이터들/word2vec자료/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(path+'Reviews.csv',nrows=100000)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 10 columns):\n",
      "Id                        100000 non-null int64\n",
      "ProductId                 100000 non-null object\n",
      "UserId                    100000 non-null object\n",
      "ProfileName               99996 non-null object\n",
      "HelpfulnessNumerator      100000 non-null int64\n",
      "HelpfulnessDenominator    100000 non-null int64\n",
      "Score                     100000 non-null int64\n",
      "Time                      100000 non-null int64\n",
      "Summary                   99998 non-null object\n",
      "Text                      100000 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 7.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
       "2  This is a confection that has been around a fe...  \"Delight\" says it all\n",
       "3  If you are looking for the secret ingredient i...         Cough Medicine\n",
       "4  Great taffy at a great price.  There was a wid...            Great taffy"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head(5)\n",
    "data.info()\n",
    "data=data[['Text','Summary']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88426"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] # text 열에 대해 중복 제외한 데이터 개수 출력\n",
    "data['Text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords=set(stopwords.words(\"english\"))\n",
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessSentence(sent, rs=True):\n",
    "    sent=sent.lower()\n",
    "    sent=BeautifulSoup(sent, 'lxml').text\n",
    "    sent=re.sub(\"\\([^)]*\\)\",\"\",sent)\n",
    "    sent=re.sub('\"',\"\",sent)\n",
    "    sent=\" \".join([contractions[t] if t in contractions else t for t in sent.split(\" \")])\n",
    "    sent=re.sub(\"'s\\b\",\"\",sent) # 소유격 제거 \\b는 왼쪽으로 커서를 이동시켜라\n",
    "    sent=re.sub('[^a-zA-Z]', ' ',sent)\n",
    "    sent=re.sub(\"[m]{2,}\",\"mm\",sent) # \n",
    "    \n",
    "    if rs:\n",
    "        tokens=\" \".join(word for word in sent.split() if not word in stopWords if len(word)>1)\n",
    "    \n",
    "    else: # 불용어 제거 안함(요약)\n",
    "        tokens=\" \".join(word for word in sent.split() if len(word)>1)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amm student'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessSentence('<a>I a\"\"mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm a student</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal', 'got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces kids husband lasted two weeks would recommend brand taffy delightful treat', 'saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party everyone loved', 'taffy good soft chewy flavors amazing would definitely recommend buying satisfying', 'right mostly sprouting cats eat grass love rotate around wheatgrass rye', 'healthy dog food good digestion also good small puppies dog eats required amount every feeding']\n"
     ]
    }
   ],
   "source": [
    "cleanText=[]\n",
    "for sent in data['Text']:\n",
    "    cleanText.append(preprocessSentence(sent))\n",
    "print(cleanText[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text']=cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good quality dog food', 'advertised', 'delight says', 'cough medicine', 'great taffy', 'nice taffy', 'great good expensive brands', 'wonderful tasty taffy', 'yay barley', 'healthy dog food']\n"
     ]
    }
   ],
   "source": [
    "cleanSummary=[]\n",
    "for sent in data['Summary']:\n",
    "    cleanSummary.append(preprocessSentence(sent))\n",
    "print(cleanSummary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary']=cleanSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99995</td>\n",
       "      <td>love buy another box done last one</td>\n",
       "      <td>yummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99996</td>\n",
       "      <td>late father law used rating system meals parti...</td>\n",
       "      <td>tastes like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99997</td>\n",
       "      <td>favorite brand korean ramen spicy used eating ...</td>\n",
       "      <td>great ramen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99998</td>\n",
       "      <td>like noodles although say spicy somewhat under...</td>\n",
       "      <td>spicy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99999</td>\n",
       "      <td>love noodle twice week amazing thing feel well...</td>\n",
       "      <td>spicy noodle cures cold upset stomach headache...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88425 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text                                            Summary\n",
       "0      bought several vitality canned dog food produc...                              good quality dog food\n",
       "1      product arrived labeled jumbo salted peanuts p...                                         advertised\n",
       "2      confection around centuries light pillowy citr...                                       delight says\n",
       "3      looking secret ingredient robitussin believe f...                                     cough medicine\n",
       "4      great taffy great price wide assortment yummy ...                                        great taffy\n",
       "...                                                  ...                                                ...\n",
       "99995                 love buy another box done last one                                              yummy\n",
       "99996  late father law used rating system meals parti...                                        tastes like\n",
       "99997  favorite brand korean ramen spicy used eating ...                                        great ramen\n",
       "99998  like noodles although say spicy somewhat under...                                              spicy\n",
       "99999  love noodle twice week amazing thing feel well...  spicy noodle cures cold upset stomach headache...\n",
       "\n",
       "[88425 rows x 2 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88134"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLen=[len(s.split()) for s in data['Text']]\n",
    "summaryLen=[len(s.split()) for s in data['Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.872115188236095"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(textLen) # 2\n",
    "np.max(textLen) # 1235\n",
    "np.mean(textLen) # 38\n",
    "np.min(summaryLen) # 1\n",
    "np.max(summaryLen) # 16\n",
    "np.mean(summaryLen) # 2,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "textMaxLen=50\n",
    "summaryMaxLen=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshLen(mlen, nlist):\n",
    "    c=0\n",
    "    for s in nlist:\n",
    "        if(len(s.split()) <=mlen):\n",
    "            c+=1\n",
    "    print(c/len(nlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7746726575441941\n",
      "None\n",
      "0.9943948986770146\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(threshLen(textMaxLen, data['Text']))\n",
    "print(threshLen(summaryMaxLen, data['Summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['Text'].apply(lambda x:len(x.split())<=textMaxLen)]\n",
    "data=data[data['Summary'].apply(lambda x:len(x.split())<=summaryMaxLen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68061\n"
     ]
    }
   ],
   "source": [
    "print(len(data)) # 68000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        sostoken good quality dog food eostoken\n",
       "1                   sostoken advertised eostoken\n",
       "2                 sostoken delight says eostoken\n",
       "3               sostoken cough medicine eostoken\n",
       "4                  sostoken great taffy eostoken\n",
       "                          ...                   \n",
       "99993              sostoken great stuff eostoken\n",
       "99994               sostoken good stuff eostoken\n",
       "99995                    sostoken yummy eostoken\n",
       "99997              sostoken great ramen eostoken\n",
       "99998                    sostoken spicy eostoken\n",
       "Name: Summary, Length: 68061, dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq2seq\n",
    "data['Summary']=data['Summary'].apply(lambda x:'sostoken '+x+' eostoken')\n",
    "data['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData=list(data['Text'])\n",
    "summaryData=list(data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest=train_test_split(textData, summaryData, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13613"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xTrain)\n",
    "len(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcToken=Tokenizer()\n",
    "srcToken.fit_on_texts(xTrain)\n",
    "# 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 1,\n",
       " 'good': 2,\n",
       " 'great': 3,\n",
       " 'taste': 4,\n",
       " 'product': 5,\n",
       " 'love': 6,\n",
       " 'one': 7,\n",
       " 'flavor': 8,\n",
       " 'coffee': 9,\n",
       " 'would': 10,\n",
       " 'tea': 11,\n",
       " 'really': 12,\n",
       " 'get': 13,\n",
       " 'amazon': 14,\n",
       " 'price': 15,\n",
       " 'best': 16,\n",
       " 'buy': 17,\n",
       " 'food': 18,\n",
       " 'much': 19,\n",
       " 'little': 20,\n",
       " 'use': 21,\n",
       " 'time': 22,\n",
       " 'find': 23,\n",
       " 'tried': 24,\n",
       " 'better': 25,\n",
       " 'also': 26,\n",
       " 'well': 27,\n",
       " 'chocolate': 28,\n",
       " 'make': 29,\n",
       " 'try': 30,\n",
       " 'eat': 31,\n",
       " 'dog': 32,\n",
       " 'even': 33,\n",
       " 'bought': 34,\n",
       " 'found': 35,\n",
       " 'delicious': 36,\n",
       " 'could': 37,\n",
       " 'order': 38,\n",
       " 'sweet': 39,\n",
       " 'cup': 40,\n",
       " 'drink': 41,\n",
       " 'tastes': 42,\n",
       " 'recommend': 43,\n",
       " 'loves': 44,\n",
       " 'bag': 45,\n",
       " 'used': 46,\n",
       " 'cannot': 47,\n",
       " 'sugar': 48,\n",
       " 'first': 49,\n",
       " 'favorite': 50,\n",
       " 'store': 51,\n",
       " 'free': 52,\n",
       " 'made': 53,\n",
       " 'nice': 54,\n",
       " 'way': 55,\n",
       " 'go': 56,\n",
       " 'box': 57,\n",
       " 'got': 58,\n",
       " 'think': 59,\n",
       " 'perfect': 60,\n",
       " 'mix': 61,\n",
       " 'water': 62,\n",
       " 'dogs': 63,\n",
       " 'since': 64,\n",
       " 'give': 65,\n",
       " 'ordered': 66,\n",
       " 'snack': 67,\n",
       " 'day': 68,\n",
       " 'bit': 69,\n",
       " 'easy': 70,\n",
       " 'two': 71,\n",
       " 'many': 72,\n",
       " 'ever': 73,\n",
       " 'hot': 74,\n",
       " 'flavors': 75,\n",
       " 'still': 76,\n",
       " 'stuff': 77,\n",
       " 'makes': 78,\n",
       " 'every': 79,\n",
       " 'never': 80,\n",
       " 'treats': 81,\n",
       " 'know': 82,\n",
       " 'quality': 83,\n",
       " 'without': 84,\n",
       " 'always': 85,\n",
       " 'healthy': 86,\n",
       " 'right': 87,\n",
       " 'brand': 88,\n",
       " 'want': 89,\n",
       " 'fresh': 90,\n",
       " 'hard': 91,\n",
       " 'tasty': 92,\n",
       " 'years': 93,\n",
       " 'definitely': 94,\n",
       " 'keep': 95,\n",
       " 'add': 96,\n",
       " 'lot': 97,\n",
       " 'something': 98,\n",
       " 'buying': 99,\n",
       " 'strong': 100,\n",
       " 'small': 101,\n",
       " 'enjoy': 102,\n",
       " 'chips': 103,\n",
       " 'old': 104,\n",
       " 'happy': 105,\n",
       " 'less': 106,\n",
       " 'wonderful': 107,\n",
       " 'excellent': 108,\n",
       " 'treat': 109,\n",
       " 'local': 110,\n",
       " 'milk': 111,\n",
       " 'bad': 112,\n",
       " 'enough': 113,\n",
       " 'say': 114,\n",
       " 'different': 115,\n",
       " 'however': 116,\n",
       " 'organic': 117,\n",
       " 'shipping': 118,\n",
       " 'need': 119,\n",
       " 'highly': 120,\n",
       " 'purchase': 121,\n",
       " 'far': 122,\n",
       " 'sure': 123,\n",
       " 'salt': 124,\n",
       " 'thought': 125,\n",
       " 'products': 126,\n",
       " 'eating': 127,\n",
       " 'size': 128,\n",
       " 'stores': 129,\n",
       " 'regular': 130,\n",
       " 'tasted': 131,\n",
       " 'looking': 132,\n",
       " 'put': 133,\n",
       " 'tasting': 134,\n",
       " 'long': 135,\n",
       " 'though': 136,\n",
       " 'pack': 137,\n",
       " 'back': 138,\n",
       " 'whole': 139,\n",
       " 'pretty': 140,\n",
       " 'using': 141,\n",
       " 'purchased': 142,\n",
       " 'bags': 143,\n",
       " 'cups': 144,\n",
       " 'high': 145,\n",
       " 'work': 146,\n",
       " 'loved': 147,\n",
       " 'bars': 148,\n",
       " 'low': 149,\n",
       " 'cookies': 150,\n",
       " 'texture': 151,\n",
       " 'ingredients': 152,\n",
       " 'package': 153,\n",
       " 'big': 154,\n",
       " 'worth': 155,\n",
       " 'grocery': 156,\n",
       " 'thing': 157,\n",
       " 'gluten': 158,\n",
       " 'green': 159,\n",
       " 'last': 160,\n",
       " 'sauce': 161,\n",
       " 'family': 162,\n",
       " 'popcorn': 163,\n",
       " 'arrived': 164,\n",
       " 'quite': 165,\n",
       " 'received': 166,\n",
       " 'year': 167,\n",
       " 'going': 168,\n",
       " 'rice': 169,\n",
       " 'see': 170,\n",
       " 'take': 171,\n",
       " 'cat': 172,\n",
       " 'item': 173,\n",
       " 'candy': 174,\n",
       " 'actually': 175,\n",
       " 'came': 176,\n",
       " 'expensive': 177,\n",
       " 'anything': 178,\n",
       " 'real': 179,\n",
       " 'feel': 180,\n",
       " 'oil': 181,\n",
       " 'dark': 182,\n",
       " 'almost': 183,\n",
       " 'chicken': 184,\n",
       " 'calories': 185,\n",
       " 'butter': 186,\n",
       " 'cereal': 187,\n",
       " 'new': 188,\n",
       " 'natural': 189,\n",
       " 'husband': 190,\n",
       " 'another': 191,\n",
       " 'full': 192,\n",
       " 'money': 193,\n",
       " 'blend': 194,\n",
       " 'trying': 195,\n",
       " 'people': 196,\n",
       " 'smell': 197,\n",
       " 'kids': 198,\n",
       " 'coconut': 199,\n",
       " 'brands': 200,\n",
       " 'getting': 201,\n",
       " 'fruit': 202,\n",
       " 'several': 203,\n",
       " 'bar': 204,\n",
       " 'disappointed': 205,\n",
       " 'half': 206,\n",
       " 'around': 207,\n",
       " 'morning': 208,\n",
       " 'absolutely': 209,\n",
       " 'home': 210,\n",
       " 'added': 211,\n",
       " 'wish': 212,\n",
       " 'amount': 213,\n",
       " 'away': 214,\n",
       " 'diet': 215,\n",
       " 'fast': 216,\n",
       " 'us': 217,\n",
       " 'usually': 218,\n",
       " 'cheese': 219,\n",
       " 'ones': 220,\n",
       " 'able': 221,\n",
       " 'fat': 222,\n",
       " 'available': 223,\n",
       " 'light': 224,\n",
       " 'deal': 225,\n",
       " 'oz': 226,\n",
       " 'come': 227,\n",
       " 'company': 228,\n",
       " 'dry': 229,\n",
       " 'bitter': 230,\n",
       " 'cats': 231,\n",
       " 'smooth': 232,\n",
       " 'foods': 233,\n",
       " 'probably': 234,\n",
       " 'said': 235,\n",
       " 'nothing': 236,\n",
       " 'may': 237,\n",
       " 'ordering': 238,\n",
       " 'kind': 239,\n",
       " 'per': 240,\n",
       " 'problem': 241,\n",
       " 'flavored': 242,\n",
       " 'breakfast': 243,\n",
       " 'liked': 244,\n",
       " 'case': 245,\n",
       " 'days': 246,\n",
       " 'works': 247,\n",
       " 'things': 248,\n",
       " 'cheaper': 249,\n",
       " 'seems': 250,\n",
       " 'save': 251,\n",
       " 'peanut': 252,\n",
       " 'black': 253,\n",
       " 'boxes': 254,\n",
       " 'bottle': 255,\n",
       " 'quick': 256,\n",
       " 'months': 257,\n",
       " 'honey': 258,\n",
       " 'fine': 259,\n",
       " 'large': 260,\n",
       " 'making': 261,\n",
       " 'gift': 262,\n",
       " 'reviews': 263,\n",
       " 'variety': 264,\n",
       " 'especially': 265,\n",
       " 'packaging': 266,\n",
       " 'quickly': 267,\n",
       " 'bread': 268,\n",
       " 'protein': 269,\n",
       " 'glad': 270,\n",
       " 'pasta': 271,\n",
       " 'others': 272,\n",
       " 'gave': 273,\n",
       " 'son': 274,\n",
       " 'recommended': 275,\n",
       " 'syrup': 276,\n",
       " 'three': 277,\n",
       " 'comes': 278,\n",
       " 'drinking': 279,\n",
       " 'amazing': 280,\n",
       " 'rich': 281,\n",
       " 'juice': 282,\n",
       " 'look': 283,\n",
       " 'energy': 284,\n",
       " 'instead': 285,\n",
       " 'house': 286,\n",
       " 'spicy': 287,\n",
       " 'top': 288,\n",
       " 'anyone': 289,\n",
       " 'prefer': 290,\n",
       " 'roast': 291,\n",
       " 'everyone': 292,\n",
       " 'meal': 293,\n",
       " 'times': 294,\n",
       " 'likes': 295,\n",
       " 'thank': 296,\n",
       " 'stars': 297,\n",
       " 'maybe': 298,\n",
       " 'wanted': 299,\n",
       " 'thanks': 300,\n",
       " 'seem': 301,\n",
       " 'pieces': 302,\n",
       " 'might': 303,\n",
       " 'cream': 304,\n",
       " 'daughter': 305,\n",
       " 'vanilla': 306,\n",
       " 'white': 307,\n",
       " 'cans': 308,\n",
       " 'extra': 309,\n",
       " 'fact': 310,\n",
       " 'month': 311,\n",
       " 'must': 312,\n",
       " 'fan': 313,\n",
       " 'baby': 314,\n",
       " 'ginger': 315,\n",
       " 'health': 316,\n",
       " 'everything': 317,\n",
       " 'next': 318,\n",
       " 'teeth': 319,\n",
       " 'either': 320,\n",
       " 'else': 321,\n",
       " 'yummy': 322,\n",
       " 'friends': 323,\n",
       " 'soft': 324,\n",
       " 'plus': 325,\n",
       " 'awesome': 326,\n",
       " 'cost': 327,\n",
       " 'enjoyed': 328,\n",
       " 'value': 329,\n",
       " 'salty': 330,\n",
       " 'nuts': 331,\n",
       " 'longer': 332,\n",
       " 'couple': 333,\n",
       " 'yet': 334,\n",
       " 'teas': 335,\n",
       " 'keurig': 336,\n",
       " 'least': 337,\n",
       " 'went': 338,\n",
       " 'crunchy': 339,\n",
       " 'ago': 340,\n",
       " 'beans': 341,\n",
       " 'bold': 342,\n",
       " 'open': 343,\n",
       " 'super': 344,\n",
       " 'online': 345,\n",
       " 'shall': 346,\n",
       " 'plastic': 347,\n",
       " 'market': 348,\n",
       " 'pleased': 349,\n",
       " 'ice': 350,\n",
       " 'decided': 351,\n",
       " 'corn': 352,\n",
       " 'started': 353,\n",
       " 'chew': 354,\n",
       " 'soup': 355,\n",
       " 'help': 356,\n",
       " 'snacks': 357,\n",
       " 'minutes': 358,\n",
       " 'took': 359,\n",
       " 'ok': 360,\n",
       " 'mouth': 361,\n",
       " 'says': 362,\n",
       " 'rather': 363,\n",
       " 'noodles': 364,\n",
       " 'cold': 365,\n",
       " 'week': 366,\n",
       " 'expected': 367,\n",
       " 'pepper': 368,\n",
       " 'cocoa': 369,\n",
       " 'flavorful': 370,\n",
       " 'alternative': 371,\n",
       " 'cookie': 372,\n",
       " 'although': 373,\n",
       " 'weight': 374,\n",
       " 'review': 375,\n",
       " 'powder': 376,\n",
       " 'machine': 377,\n",
       " 'second': 378,\n",
       " 'delivery': 379,\n",
       " 'surprised': 380,\n",
       " 'lemon': 381,\n",
       " 'ounce': 382,\n",
       " 'beef': 383,\n",
       " 'service': 384,\n",
       " 'read': 385,\n",
       " 'crackers': 386,\n",
       " 'soda': 387,\n",
       " 'opened': 388,\n",
       " 'let': 389,\n",
       " 'type': 390,\n",
       " 'goes': 391,\n",
       " 'jerky': 392,\n",
       " 'drinks': 393,\n",
       " 'wife': 394,\n",
       " 'wheat': 395,\n",
       " 'special': 396,\n",
       " 'decaf': 397,\n",
       " 'meat': 398,\n",
       " 'cooking': 399,\n",
       " 'gets': 400,\n",
       " 'cook': 401,\n",
       " 'red': 402,\n",
       " 'french': 403,\n",
       " 'plain': 404,\n",
       " 'smaller': 405,\n",
       " 'version': 406,\n",
       " 'sometimes': 407,\n",
       " 'place': 408,\n",
       " 'mixed': 409,\n",
       " 'exactly': 410,\n",
       " 'wrong': 411,\n",
       " 'aroma': 412,\n",
       " 'believe': 413,\n",
       " 'side': 414,\n",
       " 'dried': 415,\n",
       " 'shipped': 416,\n",
       " 'picky': 417,\n",
       " 'guess': 418,\n",
       " 'convenient': 419,\n",
       " 'easily': 420,\n",
       " 'packaged': 421,\n",
       " 'clean': 422,\n",
       " 'part': 423,\n",
       " 'reason': 424,\n",
       " 'hand': 425,\n",
       " 'care': 426,\n",
       " 'tell': 427,\n",
       " 'aftertaste': 428,\n",
       " 'left': 429,\n",
       " 'friend': 430,\n",
       " 'difference': 431,\n",
       " 'serving': 432,\n",
       " 'coffees': 433,\n",
       " 'refreshing': 434,\n",
       " 'fantastic': 435,\n",
       " 'artificial': 436,\n",
       " 'problems': 437,\n",
       " 'packs': 438,\n",
       " 'smells': 439,\n",
       " 'orange': 440,\n",
       " 'subscribe': 441,\n",
       " 'mild': 442,\n",
       " 'choice': 443,\n",
       " 'christmas': 444,\n",
       " 'giving': 445,\n",
       " 'slightly': 446,\n",
       " 'live': 447,\n",
       " 'container': 448,\n",
       " 'cake': 449,\n",
       " 'oatmeal': 450,\n",
       " 'cinnamon': 451,\n",
       " 'recipe': 452,\n",
       " 'spice': 453,\n",
       " 'pop': 454,\n",
       " 'potato': 455,\n",
       " 'continue': 456,\n",
       " 'pay': 457,\n",
       " 'life': 458,\n",
       " 'stomach': 459,\n",
       " 'chewy': 460,\n",
       " 'cherry': 461,\n",
       " 'starbucks': 462,\n",
       " 'often': 463,\n",
       " 'iced': 464,\n",
       " 'bulk': 465,\n",
       " 'cut': 466,\n",
       " 'almonds': 467,\n",
       " 'seeds': 468,\n",
       " 'bite': 469,\n",
       " 'recently': 470,\n",
       " 'line': 471,\n",
       " 'brown': 472,\n",
       " 'ate': 473,\n",
       " 'lots': 474,\n",
       " 'stick': 475,\n",
       " 'brew': 476,\n",
       " 'bottles': 477,\n",
       " 'compared': 478,\n",
       " 'etc': 479,\n",
       " 'huge': 480,\n",
       " 'fiber': 481,\n",
       " 'unfortunately': 482,\n",
       " 'looks': 483,\n",
       " 'dinner': 484,\n",
       " 'run': 485,\n",
       " 'looked': 486,\n",
       " 'needed': 487,\n",
       " 'end': 488,\n",
       " 'start': 489,\n",
       " 'gives': 490,\n",
       " 'carry': 491,\n",
       " 'chai': 492,\n",
       " 'seller': 493,\n",
       " 'delivered': 494,\n",
       " 'com': 495,\n",
       " 'close': 496,\n",
       " 'sent': 497,\n",
       " 'eaten': 498,\n",
       " 'hope': 499,\n",
       " 'excited': 500,\n",
       " 'night': 501,\n",
       " 'past': 502,\n",
       " 'canned': 503,\n",
       " 'larger': 504,\n",
       " 'filling': 505,\n",
       " 'calorie': 506,\n",
       " 'lunch': 507,\n",
       " 'packages': 508,\n",
       " 'items': 509,\n",
       " 'toy': 510,\n",
       " 'overall': 511,\n",
       " 'gum': 512,\n",
       " 'apple': 513,\n",
       " 'original': 514,\n",
       " 'chip': 515,\n",
       " 'waste': 516,\n",
       " 'jar': 517,\n",
       " 'simply': 518,\n",
       " 'similar': 519,\n",
       " 'heat': 520,\n",
       " 'color': 521,\n",
       " 'weeks': 522,\n",
       " 'granola': 523,\n",
       " 'idea': 524,\n",
       " 'takes': 525,\n",
       " 'soon': 526,\n",
       " 'date': 527,\n",
       " 'four': 528,\n",
       " 'broken': 529,\n",
       " 'wait': 530,\n",
       " 'salad': 531,\n",
       " 'needs': 532,\n",
       " 'saw': 533,\n",
       " 'single': 534,\n",
       " 'seasoning': 535,\n",
       " 'helps': 536,\n",
       " 'gone': 537,\n",
       " 'already': 538,\n",
       " 'feed': 539,\n",
       " 'olive': 540,\n",
       " 'someone': 541,\n",
       " 'extremely': 542,\n",
       " 'please': 543,\n",
       " 'packed': 544,\n",
       " 'creamy': 545,\n",
       " 'finally': 546,\n",
       " 'soy': 547,\n",
       " 'nut': 548,\n",
       " 'course': 549,\n",
       " 'stock': 550,\n",
       " 'crunch': 551,\n",
       " 'purchasing': 552,\n",
       " 'sweetness': 553,\n",
       " 'inside': 554,\n",
       " 'given': 555,\n",
       " 'stale': 556,\n",
       " 'weak': 557,\n",
       " 'condition': 558,\n",
       " 'non': 559,\n",
       " 'star': 560,\n",
       " 'mine': 561,\n",
       " 'healthier': 562,\n",
       " 'keeps': 563,\n",
       " 'change': 564,\n",
       " 'sold': 565,\n",
       " 'pound': 566,\n",
       " 'hooked': 567,\n",
       " 'pumpkin': 568,\n",
       " 'experience': 569,\n",
       " 'expect': 570,\n",
       " 'fish': 571,\n",
       " 'along': 572,\n",
       " 'packets': 573,\n",
       " 'espresso': 574,\n",
       " 'caffeine': 575,\n",
       " 'shipment': 576,\n",
       " 'medium': 577,\n",
       " 'pet': 578,\n",
       " 'difficult': 579,\n",
       " 'beat': 580,\n",
       " 'hours': 581,\n",
       " 'baking': 582,\n",
       " 'microwave': 583,\n",
       " 'consistency': 584,\n",
       " 'rest': 585,\n",
       " 'reasonable': 586,\n",
       " 'anywhere': 587,\n",
       " 'certainly': 588,\n",
       " 'stop': 589,\n",
       " 'locally': 590,\n",
       " 'due': 591,\n",
       " 'eats': 592,\n",
       " 'daily': 593,\n",
       " 'flour': 594,\n",
       " 'licorice': 595,\n",
       " 'cheap': 596,\n",
       " 'adding': 597,\n",
       " 'anymore': 598,\n",
       " 'hair': 599,\n",
       " 'totally': 600,\n",
       " 'together': 601,\n",
       " 'true': 602,\n",
       " 'instant': 603,\n",
       " 'shop': 604,\n",
       " 'within': 605,\n",
       " 'puppy': 606,\n",
       " 'ground': 607,\n",
       " 'training': 608,\n",
       " 'name': 609,\n",
       " 'bowl': 610,\n",
       " 'pure': 611,\n",
       " 'combination': 612,\n",
       " 'hit': 613,\n",
       " 'sodium': 614,\n",
       " 'lb': 615,\n",
       " 'done': 616,\n",
       " 'formula': 617,\n",
       " 'sale': 618,\n",
       " 'maker': 619,\n",
       " 'area': 620,\n",
       " 'banana': 621,\n",
       " 'stopped': 622,\n",
       " 'crazy': 623,\n",
       " 'leave': 624,\n",
       " 'satisfying': 625,\n",
       " 'greenies': 626,\n",
       " 'future': 627,\n",
       " 'break': 628,\n",
       " 'moist': 629,\n",
       " 'list': 630,\n",
       " 'five': 631,\n",
       " 'vet': 632,\n",
       " 'fun': 633,\n",
       " 'leaves': 634,\n",
       " 'completely': 635,\n",
       " 'ingredient': 636,\n",
       " 'chews': 637,\n",
       " 'based': 638,\n",
       " 'pricey': 639,\n",
       " 'yogurt': 640,\n",
       " 'almond': 641,\n",
       " 'entire': 642,\n",
       " 'opinion': 643,\n",
       " 'customer': 644,\n",
       " 'yes': 645,\n",
       " 'yum': 646,\n",
       " 'worked': 647,\n",
       " 'simple': 648,\n",
       " 'sour': 649,\n",
       " 'thick': 650,\n",
       " 'pick': 651,\n",
       " 'pork': 652,\n",
       " 'substitute': 653,\n",
       " 'become': 654,\n",
       " 'finding': 655,\n",
       " 'satisfied': 656,\n",
       " 'easier': 657,\n",
       " 'wow': 658,\n",
       " 'chili': 659,\n",
       " 'bland': 660,\n",
       " 'stay': 661,\n",
       " 'seemed': 662,\n",
       " 'throw': 663,\n",
       " 'today': 664,\n",
       " 'recipes': 665,\n",
       " 'sell': 666,\n",
       " 'hint': 667,\n",
       " 'mind': 668,\n",
       " 'raw': 669,\n",
       " 'pot': 670,\n",
       " 'truly': 671,\n",
       " 'okay': 672,\n",
       " 'summer': 673,\n",
       " 'dish': 674,\n",
       " 'addition': 675,\n",
       " 'world': 676,\n",
       " 'lower': 677,\n",
       " 'horrible': 678,\n",
       " 'perfectly': 679,\n",
       " 'grain': 680,\n",
       " 'decent': 681,\n",
       " 'packet': 682,\n",
       " 'maple': 683,\n",
       " 'cooked': 684,\n",
       " 'thin': 685,\n",
       " 'discovered': 686,\n",
       " 'pancakes': 687,\n",
       " 'favorites': 688,\n",
       " 'normally': 689,\n",
       " 'count': 690,\n",
       " 'finish': 691,\n",
       " 'hour': 692,\n",
       " 'later': 693,\n",
       " 'mint': 694,\n",
       " 'hoping': 695,\n",
       " 'spices': 696,\n",
       " 'contains': 697,\n",
       " 'bottom': 698,\n",
       " 'carb': 699,\n",
       " 'terrible': 700,\n",
       " 'expecting': 701,\n",
       " 'twice': 702,\n",
       " 'particular': 703,\n",
       " 'mom': 704,\n",
       " 'tiny': 705,\n",
       " 'batch': 706,\n",
       " 'mess': 707,\n",
       " 'costco': 708,\n",
       " 'none': 709,\n",
       " 'content': 710,\n",
       " 'baked': 711,\n",
       " 'newman': 712,\n",
       " 'option': 713,\n",
       " 'remember': 714,\n",
       " 'plan': 715,\n",
       " 'kick': 716,\n",
       " 'told': 717,\n",
       " 'six': 718,\n",
       " 'mixes': 719,\n",
       " 'priced': 720,\n",
       " 'piece': 721,\n",
       " 'pleasant': 722,\n",
       " 'forward': 723,\n",
       " 'paid': 724,\n",
       " 'turned': 725,\n",
       " 'prices': 726,\n",
       " 'unlike': 727,\n",
       " 'label': 728,\n",
       " 'acid': 729,\n",
       " 'grey': 730,\n",
       " 'description': 731,\n",
       " 'vinegar': 732,\n",
       " 'normal': 733,\n",
       " 'strawberry': 734,\n",
       " 'unless': 735,\n",
       " 'body': 736,\n",
       " 'noticed': 737,\n",
       " 'blue': 738,\n",
       " 'glass': 739,\n",
       " 'drinker': 740,\n",
       " 'warm': 741,\n",
       " 'seen': 742,\n",
       " 'anyway': 743,\n",
       " 'awful': 744,\n",
       " 'ready': 745,\n",
       " 'roasted': 746,\n",
       " 'dont': 747,\n",
       " 'varieties': 748,\n",
       " 'return': 749,\n",
       " 'issues': 750,\n",
       " 'impressed': 751,\n",
       " 'switch': 752,\n",
       " 'mountain': 753,\n",
       " 'bones': 754,\n",
       " 'supermarket': 755,\n",
       " 'meals': 756,\n",
       " 'carrying': 757,\n",
       " 'door': 758,\n",
       " 'fit': 759,\n",
       " 'adds': 760,\n",
       " 'veggies': 761,\n",
       " 'oh': 762,\n",
       " 'candies': 763,\n",
       " 'otherwise': 764,\n",
       " 'serve': 765,\n",
       " 'feeding': 766,\n",
       " 'loose': 767,\n",
       " 'pie': 768,\n",
       " 'bears': 769,\n",
       " 'agree': 770,\n",
       " 'stronger': 771,\n",
       " 'subscription': 772,\n",
       " 'salmon': 773,\n",
       " 'sweetener': 774,\n",
       " 'chocolates': 775,\n",
       " 'shape': 776,\n",
       " 'thinking': 777,\n",
       " 'allergies': 778,\n",
       " 'individual': 779,\n",
       " 'called': 780,\n",
       " 'overly': 781,\n",
       " 'garlic': 782,\n",
       " 'pods': 783,\n",
       " 'taking': 784,\n",
       " 'liquid': 785,\n",
       " 'sticks': 786,\n",
       " 'contain': 787,\n",
       " 'complaint': 788,\n",
       " 'breath': 789,\n",
       " 'touch': 790,\n",
       " 'job': 791,\n",
       " 'stevia': 792,\n",
       " 'carbs': 793,\n",
       " 'shake': 794,\n",
       " 'skin': 795,\n",
       " 'thrilled': 796,\n",
       " 'supply': 797,\n",
       " 'italian': 798,\n",
       " 'ship': 799,\n",
       " 'matter': 800,\n",
       " 'ask': 801,\n",
       " 'everyday': 802,\n",
       " 'earl': 803,\n",
       " 'immediately': 804,\n",
       " 'point': 805,\n",
       " 'gummy': 806,\n",
       " 'felt': 807,\n",
       " 'peanuts': 808,\n",
       " 'set': 809,\n",
       " 'heavy': 810,\n",
       " 'craving': 811,\n",
       " 'lime': 812,\n",
       " 'sealed': 813,\n",
       " 'nearly': 814,\n",
       " 'bean': 815,\n",
       " 'benefits': 816,\n",
       " 'nicely': 817,\n",
       " 'homemade': 818,\n",
       " 'chewing': 819,\n",
       " 'plenty': 820,\n",
       " 'feeling': 821,\n",
       " 'reading': 822,\n",
       " 'convenience': 823,\n",
       " 'caramel': 824,\n",
       " 'results': 825,\n",
       " 'person': 826,\n",
       " 'mother': 827,\n",
       " 'supposed': 828,\n",
       " 'uses': 829,\n",
       " 'bring': 830,\n",
       " 'style': 831,\n",
       " 'check': 832,\n",
       " 'call': 833,\n",
       " 'party': 834,\n",
       " 'sea': 835,\n",
       " 'mostly': 836,\n",
       " 'office': 837,\n",
       " 'bigger': 838,\n",
       " 'somewhat': 839,\n",
       " 'kitchen': 840,\n",
       " 'beverage': 841,\n",
       " 'crisp': 842,\n",
       " 'eggs': 843,\n",
       " 'changed': 844,\n",
       " 'except': 845,\n",
       " 'raspberry': 846,\n",
       " 'perhaps': 847,\n",
       " 'berry': 848,\n",
       " 'sick': 849,\n",
       " 'types': 850,\n",
       " 'pouch': 851,\n",
       " 'picture': 852,\n",
       " 'china': 853,\n",
       " 'share': 854,\n",
       " 'hands': 855,\n",
       " 'overpowering': 856,\n",
       " 'sorry': 857,\n",
       " 'wrapped': 858,\n",
       " 'bbq': 859,\n",
       " 'hate': 860,\n",
       " 'stuck': 861,\n",
       " 'lover': 862,\n",
       " 'kid': 863,\n",
       " 'cashews': 864,\n",
       " 'balance': 865,\n",
       " 'weird': 866,\n",
       " 'fairly': 867,\n",
       " 'worst': 868,\n",
       " 'sample': 869,\n",
       " 'usual': 870,\n",
       " 'dented': 871,\n",
       " 'afternoon': 872,\n",
       " 'send': 873,\n",
       " 'flavoring': 874,\n",
       " 'plant': 875,\n",
       " 'near': 876,\n",
       " 'pizza': 877,\n",
       " 'nutrition': 878,\n",
       " 'dishes': 879,\n",
       " 'source': 880,\n",
       " 'prepare': 881,\n",
       " 'served': 882,\n",
       " 'children': 883,\n",
       " 'brought': 884,\n",
       " 'quantity': 885,\n",
       " 'trouble': 886,\n",
       " 'addicted': 887,\n",
       " 'knew': 888,\n",
       " 'biscuits': 889,\n",
       " 'short': 890,\n",
       " 'means': 891,\n",
       " 'alone': 892,\n",
       " 'blood': 893,\n",
       " 'grams': 894,\n",
       " 'mango': 895,\n",
       " 'kept': 896,\n",
       " 'bake': 897,\n",
       " 'kinds': 898,\n",
       " 'gourmet': 899,\n",
       " 'reviewers': 900,\n",
       " 'gotten': 901,\n",
       " 'running': 902,\n",
       " 'replacement': 903,\n",
       " 'whatever': 904,\n",
       " 'bodied': 905,\n",
       " 'heard': 906,\n",
       " 'ended': 907,\n",
       " 'higher': 908,\n",
       " 'chance': 909,\n",
       " 'working': 910,\n",
       " 'alot': 911,\n",
       " 'pounds': 912,\n",
       " 'expiration': 913,\n",
       " 'lbs': 914,\n",
       " 'issue': 915,\n",
       " 'pleasantly': 916,\n",
       " 'vegetables': 917,\n",
       " 'coming': 918,\n",
       " 'reviewer': 919,\n",
       " 'double': 920,\n",
       " 'website': 921,\n",
       " 'sitting': 922,\n",
       " 'enjoys': 923,\n",
       " 'unique': 924,\n",
       " 'paying': 925,\n",
       " 'shelf': 926,\n",
       " 'walmart': 927,\n",
       " 'crispy': 928,\n",
       " 'jelly': 929,\n",
       " 'number': 930,\n",
       " 'dressing': 931,\n",
       " 'jars': 932,\n",
       " 'air': 933,\n",
       " 'fill': 934,\n",
       " 'dessert': 935,\n",
       " 'restaurant': 936,\n",
       " 'blends': 937,\n",
       " 'pour': 938,\n",
       " 'hold': 939,\n",
       " 'tart': 940,\n",
       " 'clear': 941,\n",
       " 'portion': 942,\n",
       " 'disappointing': 943,\n",
       " 'careful': 944,\n",
       " 'covered': 945,\n",
       " 'eater': 946,\n",
       " 'standard': 947,\n",
       " 'brewed': 948,\n",
       " 'mustard': 949,\n",
       " 'sensitive': 950,\n",
       " 'cause': 951,\n",
       " 'bits': 952,\n",
       " 'birthday': 953,\n",
       " 'helped': 954,\n",
       " 'listed': 955,\n",
       " 'beer': 956,\n",
       " 'personally': 957,\n",
       " 'school': 958,\n",
       " 'notice': 959,\n",
       " 'gf': 960,\n",
       " 'lasts': 961,\n",
       " 'busy': 962,\n",
       " 'sauces': 963,\n",
       " 'sort': 964,\n",
       " 'lab': 965,\n",
       " 'switched': 966,\n",
       " 'surprise': 967,\n",
       " 'selling': 968,\n",
       " 'potatoes': 969,\n",
       " 'threw': 970,\n",
       " 'site': 971,\n",
       " 'cool': 972,\n",
       " 'offered': 973,\n",
       " 'fair': 974,\n",
       " 'gummi': 975,\n",
       " 'sized': 976,\n",
       " 'cracker': 977,\n",
       " 'note': 978,\n",
       " 'brewing': 979,\n",
       " 'bitterness': 980,\n",
       " 'trip': 981,\n",
       " 'earth': 982,\n",
       " 'beautiful': 983,\n",
       " 'nasty': 984,\n",
       " 'grains': 985,\n",
       " 'sells': 986,\n",
       " 'tree': 987,\n",
       " 'tomato': 988,\n",
       " 'tuna': 989,\n",
       " 'directions': 990,\n",
       " 'turn': 991,\n",
       " 'slight': 992,\n",
       " 'putting': 993,\n",
       " 'hazelnut': 994,\n",
       " 'avoid': 995,\n",
       " 'enjoying': 996,\n",
       " 'consider': 997,\n",
       " 'states': 998,\n",
       " 'lost': 999,\n",
       " 'compare': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcToken.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54448\n"
     ]
    }
   ],
   "source": [
    "print(srcToken.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCnt=len(srcToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도수 총합 1323945\n",
      "단어수 32668\n",
      "0.7870699155136525\n",
      "0.04192696826529803\n",
      "6956\n"
     ]
    }
   ],
   "source": [
    "totalFreq=0 # 전체 단어 빈도수 총 합\n",
    "rCnt=0 # 빈도수가 10 미만인 단어의 개수\n",
    "rFreq=0 # 빈도수가 10 미만인 단어빈도수 총 합\n",
    "for k,v in srcToken.word_counts.items():\n",
    "    totalFreq+=v\n",
    "    if (v<10): \n",
    "        rCnt+=1\n",
    "        rFreq+=v\n",
    "print(\"단어 빈도수 총합\", totalFreq)\n",
    "print(\"단어수\",totalCnt)\n",
    "print(rCnt/totalCnt)\n",
    "print(rFreq/totalFreq)\n",
    "print(totalCnt-rCnt) # 6956 -> 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcVocab=7000\n",
    "srcToken=Tokenizer(num_words=srcVocab)\n",
    "srcToken.fit_on_texts(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=srcToken.texts_to_sequences(xTrain)\n",
    "xTest=srcToken.texts_to_sequences(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105, 170, 14, 491, 523, 7, 71, 1106, 45, 249, 1705, 170, 51, 33, 534, 1479, 718, 143, 168, 160, 135]\n"
     ]
    }
   ],
   "source": [
    "print(xTrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarToken=Tokenizer()\n",
    "tarToken.fit_on_texts(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도수 총합 254334\n",
      "단어수 32668\n",
      "0.28223337822946004\n",
      "0.07756336156392775\n",
      "23448\n"
     ]
    }
   ],
   "source": [
    "totalFreq=0 # 전체 단어 빈도수 총 합\n",
    "rCnt=0 # 빈도수가 10 미만인 단어의 개수\n",
    "rFreq=0 # 빈도수가 10 미만인 단어빈도수 총 합\n",
    "for k,v in tarToken.word_counts.items():\n",
    "    totalFreq+=v\n",
    "    if (v<10): \n",
    "        rCnt+=1\n",
    "        rFreq+=v\n",
    "print(\"단어 빈도수 총합\", totalFreq) # 254334\n",
    "print(\"단어수\",totalCnt) # 32668\n",
    "print(rCnt/totalCnt) # 28\n",
    "print(rFreq/totalFreq) # 7\n",
    "print(totalCnt-rCnt) # 6956 -> 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarVoc=2000\n",
    "tarTokenizer=Tokenizer(num_words=tarVoc)\n",
    "tarTokenizer.fit_on_texts(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain=tarTokenizer.texts_to_sequences(yTrain)\n",
    "yTest=tarTokenizer.texts_to_sequences(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 44, 5, 2],\n",
       " [1, 16, 2],\n",
       " [1, 10, 735, 8, 2],\n",
       " [1, 33, 801, 85, 1300, 2],\n",
       " [1, 187, 234, 183, 54, 31, 2]]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropTrain=[i for i, sent in enumerate(yTrain) if len(sent)==2 ]\n",
    "dropTest=[i for i, sent in enumerate(yTest) if len(sent)==2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=np.delete(xTrain, dropTrain, axis=0)\n",
    "yTrain=np.delete(yTrain, dropTrain, axis=0)\n",
    "xTest=np.delete(xTest, dropTest, axis=0)\n",
    "yTest=np.delete(yTest, dropTest, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13181"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xTrain) # 52722\n",
    "len(xTest) # 13181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=pad_sequences(xTrain, maxlen=textMaxLen, padding='post')\n",
    "xTest=pad_sequences(xTest, maxlen=textMaxLen, padding='post')\n",
    "yTrain=pad_sequences(yTrain, maxlen=summaryMaxLen, padding='post')\n",
    "yTest=pad_sequences(yTest, maxlen=summaryMaxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      896000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,505,104\n",
      "Trainable params: 3,505,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(textMaxLen,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(srcVocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb = Embedding(tarVoc, embedding_dim)(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tarVoc, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      896000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,148,432\n",
      "Trainable params: 4,148,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 52722 samples, validate on 13181 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-2e19095dfac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m history = model.fit([xTrain, yTrain[:,:-1]], yTrain.reshape(yTrain.shape[0], yTrain.shape[1], 1)[:,1:] \\\n\u001b[0;32m     24\u001b[0m                   ,epochs=50, callbacks=[es], batch_size = 256, validation_data=([xTest, yTest[:,:-1]], \\\n\u001b[1;32m---> 25\u001b[1;33m                   yTest.reshape(yTest.shape[0], yTest.shape[1], 1)[:,1:]))\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tarVoc, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit([xTrain, yTrain[:,:-1]], yTrain.reshape(yTrain.shape[0], yTrain.shape[1], 1)[:,1:] \\\n",
    "                  ,epochs=50, callbacks=[es], batch_size = 256, validation_data=([xTest, yTest[:,:-1]], \\\n",
    "                  yTest.reshape(yTest.shape[0], yTest.shape[1], 1)[:,1:]))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ragged'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-8f84900e402f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_seq2seq_Multi_LSTM.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msrc_index_to_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrcToken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m \u001b[1;31m# 원문 단어 집합에서 정수 -> 단어를 얻음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtar_word_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarToken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m \u001b[1;31m# 요약 단어 집합에서 단어 -> 정수를 얻음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    625\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 147\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[1;32m-> 1042\u001b[1;33m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \"\"\"\n\u001b[1;32m-> 1179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ragged'"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "src_index_to_word = srcToken.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tarToken.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tarToken.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(textMaxLen, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tarVoce))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > maxTarLen):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostoken']) and i!=target_word_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "for i in range(500, 1000):\n",
    "    print(\"원문 : \",seq2text(xTest[i]))\n",
    "    print(\"실제 요약문 :\",seq2summary(yTest[i]))\n",
    "    print(\"예측 요약문 :\",decode_sequence(xTest[i].reshape(1, textMaxLen)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('review_seq2seq_Multi_LSTM.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: AttentionLayer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-2c34877e9f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'review_seq2seq_Multi_LSTM.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    141\u001b[0m   if (h5py is not None and (\n\u001b[0;32m    142\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[1;32m--> 162\u001b[1;33m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;31m# set weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    189\u001b[0m             custom_objects=dict(\n\u001b[0;32m    190\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                 list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m       \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     \u001b[1;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m   1053\u001b[0m       \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[1;32m--> 180\u001b[1;33m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown layer: AttentionLayer"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('review_seq2seq_Multi_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
