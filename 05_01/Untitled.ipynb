{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct=np.arange(7*5*2).reshape(7,5,2)\n",
    "np.transpose(ct).shape\n",
    "# m*n => .T => n*m\n",
    "# i*j*k => .T => ikj, jik, jki, kij, kji\n",
    "np.transpose(ct, [1,2,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 14 15 16]\n",
      " [17 18 19 20]\n",
      " [21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "t=[i for i in range(1,25)]\n",
    "t=tf.reshape(t,[2,3,4])\n",
    "t=t[-1]\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast study 1 : 4글자로 구성된 단어 -> 학습\n",
    "# 단어의 앞 3글자 입력 -> 마지막 글자 예측\n",
    "# wood -> 학습 -> 모델\n",
    "# woo 입력 -----> 모델 -> d 예측\n",
    "# wop 입력 -----> 모델 -> d 예측\n",
    "\n",
    "char_arr=[chr(i) for i in range(97,123)] # 'a' ~ 'z' \n",
    "# num_dic={}\n",
    "# for i in range(len(char_arr)):\n",
    "#     num_dic[char_arr[i]]=i\n",
    "    \n",
    "num_dic={n:i for i, n in enumerate(char_arr)}\n",
    "dic_len=len(num_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data=['word','wood','deep','dive','cold','cool','load','love','kiss','kind',]\n",
    "# x: wor, y:d\n",
    "# x: woo, y:d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch=[]\n",
    "    target_batch=[]\n",
    "    for seq in seq_data: # seq = word\n",
    "        myInput=[num_dic[n] for n in seq[:-1]] # wor => 22, 14, 17\n",
    "        #print(myInput)\n",
    "        target=num_dic[seq[-1]]\n",
    "        input_batch.append(np.eye(dic_len)[myInput])\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch # x data, y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(seq_data)\n",
    "# woo, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 15, 4, 3, 11, 3, 4, 18, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_cross_entropy_with_logits 함수는 label 값을 one-hot 인코딩으로 넘겨줌\n",
    "# sparse_softmax_cross_entropy_with_logits 함수는 label 값이 one-hot 인코딩이 안된 경우에 사용\n",
    "lr=0.01\n",
    "n_hidden=128 # cell 출력 개수\n",
    "total_epoch=30\n",
    "n_step=3 # 입력 3글자\n",
    "# 일반적으로 단어들은 4글자가 아님 => 때문에 training할 단어중 가장 긴 단어를 기준으로 \n",
    "# 다른 짧은 단어들에게는 0을 넣어서 padding해줌\n",
    "\n",
    "n_input=n_class=dic_len # 26, 입력 크기(n_input), 클래스 종류 개수(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-6f51219e7841>:8: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-6f51219e7841>:11: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-6f51219e7841>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 구성\n",
    "x=tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "# 단어의 개수,  n_step : 3글자 입력, n_input : 26개 종류\n",
    "y=tf.placeholder(tf.int32, [None]) \n",
    "# y가 ohe 되어 있다면 -> [None, n_class]\n",
    "w=tf.Variable(tf.random_normal([n_hidden,n_class]))\n",
    "b=tf.Variable(tf.random_normal([n_class])) # 26글자여서 26\n",
    "cell1=tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "cell1=tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "cell2=tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "multi_cell=tf.nn.rnn_cell.MultiRNNCell([cell1,cell2]) # cell1, cell2 를 아래 위로 연결시켜 줌\n",
    "outputs, states=tf.nn.dynamic_rnn(multi_cell, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=tf.transpose(outputs, [1,0,2])\n",
    "# 10, 3, 128 => 3, 10, 128\n",
    "outputs=outputs[-1] # 10, 128\n",
    "model=tf.matmul(outputs, w)+b\n",
    "# 10, 128 [128, 26] => 10, 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=model, labels=y\n",
    "    )\n",
    ")\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(total_epoch):\n",
    "    _, cv=sess.run([opt, cost],feed_dict={x:input_batch, y:target_batch})\n",
    "    print(\"에폭:\",\"%04d\"%(epoch+1),\"비용:\",\"{:.5f}\".format(cv))\n",
    "print(\"모델 작성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "predCheck=tf.equal(pred,y)\n",
    "accuracy=tf.reduce_mean(tf.cast(predCheck, tf.float32))\n",
    "\n",
    "input_batch, target_batch=make_batch(seq_data)\n",
    "pv,av=sess.run([pred, accuracy], feed_dict={x:input_batch, y:target_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words=[]\n",
    "for i, v in enumerate(seq_data):\n",
    "    last_char=char_arr[pv[i]]\n",
    "    predict_words.append(v[:3]+last_char)\n",
    "print(\"에측결과\\n\")\n",
    "print(\"입력값:\",[w[:3] for w in seq_data])\n",
    "print(\"예측값:\",predict_words)\n",
    "print(\"정확도:\",av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq : 기계번연, \n",
    "# 입력신경망(encoder) / 출력신경망(decoder)\n",
    "# (ex) 나는 학교에 간다 -> i go to school .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq : 챗봇, 본역 ,이미지 캡셔닝 등에 활용\n",
    "# 영어 단어 -> 한국어 단어 번역기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "S : 인코딩 입력의 시작\n",
    "E : 디코딩 출력의 끝\n",
    "P : 배치되는 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
    "\n",
    "배치 데이터의 최대 크기 4인 경우\n",
    "word => ['w','o','r','d']\n",
    "to => ['t','o','P','P']\n",
    "\"\"\"\n",
    "char_arr=[c for c in \"SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑\"]\n",
    "num_dic={n:i for i, n in enumerate(char_arr)} # i => index, n : 해당 단어\n",
    "dic_len=len(num_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data=[['word','단어'],['wood','나무'],['game','놀이'],['girl','소녀'],['love','사랑'],['kiss','키스']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch=[]\n",
    "    output_batch=[]\n",
    "    target_batch=[]\n",
    "    for seq in seq_data:\n",
    "        input_data=[num_dic[n] for n in seq[0]]\n",
    "        # print(input_data)\n",
    "        output_data= [num_dic[n] for n in ('S'+seq[1])]\n",
    "        target_data=[num_dic[n] for n in (seq[1]+'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        output_batch.append(np.eye(dic_len)[output_data])\n",
    "        target_batch.append(target_data) # 출력값만 ohe가 아님\n",
    "    \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch,output_batch,target_batch=make_batch(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 옵션 설정\n",
    "lr = 0.01\n",
    "n_hidden=128\n",
    "total_epoch=100\n",
    "n_class=n_input=dic_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성=[배치사이즈, 단계, 입력크기]\n",
    "encInput=tf.placeholder(tf.float32, [None,None,n_input])   # word\n",
    "decInput=tf.placeholder(tf.float32, [None,None,n_input])   # <S>단어\n",
    "targets=tf.placeholder(tf.int64, [None,None])   # 단어<E>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 셀울 구성\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell=tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    outputs, enc_states=tf.nn.dynamic_rnn(enc_cell, encInput, dtype=tf.float32)\n",
    "# enc_states 가 다음 decoder에게 입력으로 전달되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 셀 구성\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell=tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell=tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states=tf.nn.dynamic_rnn(dec_cell, decInput, initial_state=enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.layers.dense(outputs, n_class, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=model, labels=targets\n",
    "))\n",
    "opt=tf.train.AdamOptimizer(lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, output_batch, target_batch=make_batch(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(total_epoch):\n",
    "    _,cv=sess.run([opt, cost], feed_dict={\n",
    "            encInput:input_batch,\n",
    "            decInput:output_batch,\n",
    "            targets:target_batch\n",
    "        }\n",
    "    )\n",
    "    print(\"에폭:\",\"%04d\"%(epoch+1),\"비용:\",\"{:.5f}\".format(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(w):\n",
    "    seq_data=[w, 'P'*len(w)]\n",
    "    #['love', 'PPPP']\n",
    "    input_batch, output_batch, target_batch=make_batch([seq_data])\n",
    "    # input_batch=['w','o','r','d']\n",
    "    # output_batch=['P','P','P','P']\n",
    "    # target_batch=['P','P','P','P'] -> [2,2,2,2]    \n",
    "    \n",
    "    # model 실행결과 : [배치사이즈 , 스탭, 입력크기]\n",
    "    # 2번째 차원인 입력 차원을 argmax 적용 -> 확률이 가장 높은 글자가 예측이 되도록\n",
    "    prediction=tf.argmax(model,2)\n",
    "    #[[[0,0,0.9,0.5.........,0.1]]] # 확률이 들어감\n",
    "    # => [[[2]]]\n",
    "    \n",
    "    res=sess.run(prediction, feed_dict={\n",
    "        encInput:input_batch,\n",
    "        decInput:output_batch,\n",
    "        targets:target_batch\n",
    "    })\n",
    "    # res에는 숫자 인덱스\n",
    "    decoded=[char_arr[i] for i in res[0]]\n",
    "    end=decoded.index('E')\n",
    "    translated=\"\".join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word?\",translate('word'))\n",
    "print(\"wodr?\",translate('wodr'))\n",
    "print(\"love?\",translate('love'))\n",
    "print(\"loev?\",translate('loev'))\n",
    "print(\"like?\",translate('like'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
