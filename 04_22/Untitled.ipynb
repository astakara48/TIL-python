{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# pip install tqdm\n",
    "from tqdm import tqdm\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/student/Downloads/데이터들/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingEpochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, [None, 28*28])\n",
    "y=tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "w1=tf.get_variable(\"w1\",shape=[784,256], initializer=tf.contrib.layers.xavier_initializer()) # xavier 알고리즘으로 weight 초기화\n",
    "#w1=tf.get_variable(\"w1\",shape=[784,256], initializer=tf.keras.initializers.he_normal())\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "l1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\",shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#w2=tf.get_variable(\"w2\",shape=[256,256], initializer=tf.keras.initializers.he_normal())\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "l2=tf.nn.relu(tf.matmul(l1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\",shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#w3=tf.get_variable(\"w3\",shape=[256,256], initializer=tf.keras.initializers.he_normal())\n",
    "b3=tf.Variable(tf.random_normal([256]))\n",
    "l3=tf.nn.relu(tf.matmul(l2,w3)+b3)\n",
    "\n",
    "w4=tf.get_variable(\"w4\",shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#w4=tf.get_variable(\"w4\",shape=[256,256], initializer=tf.keras.initializers.he_normal())\n",
    "b4=tf.Variable(tf.random_normal([256]))\n",
    "l4=tf.nn.relu(tf.matmul(l3,w4)+b4)\n",
    "\n",
    "w5=tf.get_variable(\"w5\",shape=[256,10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#w5=tf.get_variable(\"w1\",shape=[256,10], initializer=tf.kreas.initializers.he_normal())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hf=tf.matmul(l4,w5)+b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavior 알고리즘\n",
    "# np.random.randn(입력노드수, 출력노드수)/np.sqrt(입력노드수)\n",
    "#tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# he 알고리즘\n",
    "# np.random.randn(입력노드수, 출력노드수)/np.sqrt(입력노드수/2)\n",
    "#tf.keras.initializers.he_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hf, labels=y))\n",
    "optimizer= tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "prediction=tf.equal(tf.argmax(hf,1), tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "# 가중치 업데이트 방법\n",
    "\n",
    "# 전통적 경사하강법은 매번 미분함 => 속도 느림, 비효과적인 최적해 찾기\n",
    "# 확률적 경사하강법(sgd) => 속도 개선\n",
    "# 모멘텀(momentum) sgd => 확률적 경사하강법 + 관성\n",
    "# 아다그리드 : step size 조절 => 학습을 빠르게\n",
    "# rmsprop : 아다그리드 개선 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(trainingEpochs):\n",
    "    avgCost=0\n",
    "    totalBatch=int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    pbar=tqdm(range(totalBatch))\n",
    "    \n",
    "    for i in pbar:\n",
    "        batchX,batchY=mnist.train.next_batch(batch_size)\n",
    "        cv,_=sess.run([cost,optimizer], feed_dict={x:batchX, y:batchY},keep_prob:0.7) \n",
    "        # keep_prob : 여기서는 100개를 뽑았는데 그중 70개 랜덤하게 뽑아서 그것으로 트레이닝 시킴 / 모델 만들떄 사용해야함\n",
    "        # 테스트할떄는 1을 줘야 함\n",
    "        avgCost+=cv/totalBatch\n",
    "        pbar.set_description('cost:%f'%avgCost)\n",
    "print(\"정확도:\", sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=sess.run(tf.argmax(mnist.test.labels,1))\n",
    "predictions=sess.run(tf.argmax(hf,1), feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "index=[]\n",
    "ori=[]\n",
    "pred=[]\n",
    "for i in range(0, mnist.test.num_examples):\n",
    "    if predictions[i]!=labels[i]:\n",
    "        index.append(i)\n",
    "        ori.append(labels[i])\n",
    "        pred.append(predictions[i])\n",
    "res=pd.DataFrame({\"label\":ori, \"predict\":pred, \"index\":index})\n",
    "print(res)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(res['predict'], bins=10)\n",
    "plt.xlabel('fault prediction')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.violinplot(x='label',y='predict', data=res)\n",
    "plt.xlabel('fault prediction')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.swarmplot(x='label',y='predict', data=res)\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xlabel('fault prediction')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver=res.query(\"label==4\").sample(n=8).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for n in ver:\n",
    "    cnt+=1\n",
    "    plt.subplot(4,4,cnt)\n",
    "    plt.imshow(mnist.test.images[n].reshape(28,28), cmap='Greys')\n",
    "    t=\"label:\"+str(res['label'][n])+\"pred::\"+str(res['predict'][n])\n",
    "    plt.title(t)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pimna 인디언 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 생성\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.loadtxt('C:/Users/student/Downloads/데이터들/dataset/pima-indians-diabetes.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[:,0:8]\n",
    "y=data[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 떄는 categorical_crossentropy , binary classification이면 바이너리 저거\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x,y,epochs=200,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x,y) # cost, 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(trainImage,trainLabel),(testImage,testLabel)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit=trainImage[3]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(512, input_shape=(28*28,), activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImage=trainImage.reshape(60000,28*28)\n",
    "trainImage=trainImage.astype('float32')/255\n",
    "testImage=testImage.reshape(10000,28*28)\n",
    "testImage=testImage.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "trainLabel=to_categorical(trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabel=to_categorical(testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(trainImage, trainLabel, epochs=5, batch_size=128) # 2^n 으로 주는게 좋음 뭐든지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, a=model2.evaluate(testImage, testLabel)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "일반적(텐서플로) : 이미지(128, 높이, 너비, 3(채널))\n",
    "씨아노 : 이미지(128, 채널, 높이, 너비)\n",
    "\n",
    "비디오 : 5차원 텐서, 프레임의 연속\n",
    "(프레임, 높이, 너비, 채널)\n",
    "ex) 60초 짜리, 100*200 유튜브 비디오, 초당 4프레임을 샘플링\n",
    "4*60=240 프레임\n",
    "비디오 클립이 5개\n",
    "(5, 240, 100, 200, 3(color))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData,trainLabel),(testData, testLabel)=imdb.load_data(num_words=10000)\n",
    "# 자주 등장하는 10000개 단어만 추출해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape # 25000,\n",
    "trainData[0]\n",
    "trainLabel[0] # 1 -> 긍정, 0 -> 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([len(seq) for seq in trainData]) # 각각의 리뷰에 사용되어진 단어의 개수\n",
    "print(max([max(seq) for seq in trainData]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex=imdb.get_word_index() # 단어와 단어에 해당하는 숫자를 매핑\n",
    "wordIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_wordIndex=dict([(value, key) for (key, value) in wordIndex.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decReview=\" \".join([rev_wordIndex.get(i-3, \"?\") for i in trainData[0]])\n",
    "# 0,1,2,3은 '',and,the,a 라서 필요가 없음 => 그래서 3개 단어는 그냥 ? 처리를 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 길이를 동일하게 하는 작업 : 패딩\n",
    "# 동일한 길이의 리스가 되도록 패딩 작업수행\n",
    "# ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_seq(data, dim=10000):\n",
    "    res=np.zeros((len(data),dim))\n",
    "    for i, s in enumerate(data):\n",
    "        res[i,s]=1\n",
    "    return res\n",
    "    \n",
    "xTrain=vec_seq(trainData)\n",
    "xTest=vec_seq(testData)\n",
    "#훈련데이터-> 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain=trainLabel.astype(\"float32\")\n",
    "yTest=testLabel.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델\n",
    "model=Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(10000,))) # 출력:16 , relu, Dense 형태\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVal=xTrain[:10000]\n",
    "p_xTrain=xTrain[10000:]\n",
    "yVal=yTrain[:10000]\n",
    "p_yTrain=yTrain[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(p_xTrain, p_yTrain, epochs=20, batch_size=512, validation_data=(xVal, yVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyDict=history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo=range(1,len(acc)+1)\n",
    "plt.plot(epo, loss, 'bo', label='Training loss')\n",
    "plt.plot(epo, val_loss, 'b', label='Val loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epo, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epo, val_acc, 'b', label='Val accuracy')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번\n",
    "# 실제 집 가격와 예측한 집 가격의 차이가 얼마나 나는지?\n",
    "# loss='mae', optimizer='rmsprop', metrics=['mae'](평균절대오차)\n",
    "# keras 버전, tensor 버전 2지로 만들기\n",
    "\n",
    "\n",
    "# 2번\n",
    "# iris.csv 아이리스 품종 분류기\n",
    "# loss=categorical_crossentropy, optimizer='adam', metrics=['accuracy']\n",
    "# keras 버전, tensor 버전\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData,trainTargets),(testData, testTarget)=boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=trainData[:283]\n",
    "xVal=trainData[283:]\n",
    "yTrain=trainTargets[:283]\n",
    "yVal=trainTargets[283:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape # 404, 13\n",
    "trainTargets # 집 가격\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(64, activation='relu', input_shape=(13,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 283 samples, validate on 121 samples\n",
      "Epoch 1/100\n",
      "283/283 [==============================] - ETA: 29s - loss: 2715.9172 - mae: 52.11 - ETA: 0s - loss: 243.0863 - mae: 11.9345 - ETA: 0s - loss: 180.1317 - mae: 10.06 - ETA: 0s - loss: 169.7522 - mae: 9.6650 - ETA: 0s - loss: 150.7191 - mae: 9.194 - 0s 2ms/step - loss: 146.8660 - mae: 8.9981 - val_loss: 110.3103 - val_mae: 8.9046\n",
      "Epoch 2/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 78.8919 - mae: 8.88 - ETA: 0s - loss: 123.6996 - mae: 7.619 - ETA: 0s - loss: 114.1618 - mae: 8.082 - ETA: 0s - loss: 94.0429 - mae: 7.177 - ETA: 0s - loss: 96.3762 - mae: 7.17 - ETA: 0s - loss: 92.4516 - mae: 6.99 - 0s 1ms/step - loss: 89.7088 - mae: 6.9279 - val_loss: 131.1029 - val_mae: 7.3270\n",
      "Epoch 3/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.9430 - mae: 0.971 - ETA: 0s - loss: 80.8707 - mae: 6.45 - ETA: 0s - loss: 82.5029 - mae: 6.36 - ETA: 0s - loss: 78.3275 - mae: 6.28 - ETA: 0s - loss: 80.1880 - mae: 6.36 - 0s 1ms/step - loss: 77.2587 - mae: 6.3255 - val_loss: 112.3198 - val_mae: 6.5408\n",
      "Epoch 4/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 60.5053 - mae: 7.77 - ETA: 0s - loss: 62.3287 - mae: 5.82 - ETA: 0s - loss: 66.9808 - mae: 5.73 - ETA: 0s - loss: 70.5209 - mae: 5.96 - ETA: 0s - loss: 63.6712 - mae: 5.74 - 0s 1ms/step - loss: 62.7597 - mae: 5.6922 - val_loss: 90.8300 - val_mae: 7.4195\n",
      "Epoch 5/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.1834 - mae: 1.087 - ETA: 0s - loss: 42.6894 - mae: 4.54 - ETA: 0s - loss: 52.0861 - mae: 5.14 - ETA: 0s - loss: 55.0066 - mae: 5.17 - ETA: 0s - loss: 57.8916 - mae: 5.41 - ETA: 0s - loss: 56.2295 - mae: 5.38 - 0s 1ms/step - loss: 56.1371 - mae: 5.4072 - val_loss: 83.2038 - val_mae: 6.6928\n",
      "Epoch 6/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 42.1757 - mae: 6.49 - ETA: 0s - loss: 41.4105 - mae: 4.57 - ETA: 0s - loss: 55.7787 - mae: 5.25 - ETA: 0s - loss: 60.8864 - mae: 5.36 - ETA: 0s - loss: 56.7629 - mae: 5.28 - 0s 982us/step - loss: 56.0229 - mae: 5.2533 - val_loss: 119.2490 - val_mae: 7.1086\n",
      "Epoch 7/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 49.6822 - mae: 7.04 - ETA: 0s - loss: 70.5607 - mae: 5.96 - ETA: 0s - loss: 60.8888 - mae: 5.28 - ETA: 0s - loss: 49.6086 - mae: 4.97 - ETA: 0s - loss: 54.4237 - mae: 5.17 - 0s 1ms/step - loss: 50.9055 - mae: 5.0108 - val_loss: 85.1698 - val_mae: 5.6489\n",
      "Epoch 8/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 4.5642 - mae: 2.136 - ETA: 0s - loss: 51.0940 - mae: 5.18 - ETA: 0s - loss: 50.8521 - mae: 5.50 - ETA: 0s - loss: 48.3501 - mae: 5.30 - ETA: 0s - loss: 50.0018 - mae: 5.36 - 0s 1ms/step - loss: 46.4354 - mae: 5.1644 - val_loss: 75.8565 - val_mae: 5.5397\n",
      "Epoch 9/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.5283 - mae: 1.236 - ETA: 0s - loss: 41.8727 - mae: 4.77 - ETA: 0s - loss: 37.2272 - mae: 4.67 - ETA: 0s - loss: 45.7023 - mae: 4.85 - ETA: 0s - loss: 43.9098 - mae: 4.80 - 0s 1ms/step - loss: 45.1156 - mae: 4.8588 - val_loss: 95.2985 - val_mae: 6.0968\n",
      "Epoch 10/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.1006 - mae: 2.846 - ETA: 0s - loss: 41.1950 - mae: 4.58 - ETA: 0s - loss: 42.9840 - mae: 4.64 - ETA: 0s - loss: 45.3999 - mae: 4.65 - ETA: 0s - loss: 42.0137 - mae: 4.56 - 0s 1ms/step - loss: 43.8332 - mae: 4.6162 - val_loss: 65.6995 - val_mae: 6.3655\n",
      "Epoch 11/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2199 - mae: 1.489 - ETA: 0s - loss: 49.4525 - mae: 4.91 - ETA: 0s - loss: 44.1116 - mae: 4.69 - ETA: 0s - loss: 46.2331 - mae: 4.88 - ETA: 0s - loss: 42.5495 - mae: 4.76 - 0s 979us/step - loss: 40.9219 - mae: 4.7095 - val_loss: 64.4998 - val_mae: 5.1902\n",
      "Epoch 12/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.8666 - mae: 0.930 - ETA: 0s - loss: 40.5685 - mae: 4.55 - ETA: 0s - loss: 48.4548 - mae: 5.00 - ETA: 0s - loss: 44.8965 - mae: 4.78 - ETA: 0s - loss: 40.7889 - mae: 4.62 - 0s 1ms/step - loss: 40.3975 - mae: 4.6511 - val_loss: 66.0651 - val_mae: 5.0930\n",
      "Epoch 13/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 31.3780 - mae: 5.60 - ETA: 0s - loss: 42.8791 - mae: 5.01 - ETA: 0s - loss: 44.9432 - mae: 4.99 - ETA: 0s - loss: 42.6856 - mae: 4.82 - ETA: 0s - loss: 41.6745 - mae: 4.72 - 0s 1ms/step - loss: 39.5800 - mae: 4.6193 - val_loss: 130.9033 - val_mae: 9.5261\n",
      "Epoch 14/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 222.5412 - mae: 14.91 - ETA: 0s - loss: 73.3348 - mae: 5.9928 - ETA: 0s - loss: 49.9838 - mae: 5.03 - ETA: 0s - loss: 45.2618 - mae: 4.82 - ETA: 0s - loss: 41.0072 - mae: 4.63 - 0s 1ms/step - loss: 40.1662 - mae: 4.5985 - val_loss: 72.0264 - val_mae: 5.2250\n",
      "Epoch 15/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.2991 - mae: 1.139 - ETA: 0s - loss: 48.1250 - mae: 4.76 - ETA: 0s - loss: 45.0698 - mae: 4.85 - ETA: 0s - loss: 43.1776 - mae: 4.76 - ETA: 0s - loss: 40.9084 - mae: 4.64 - 0s 1ms/step - loss: 37.9764 - mae: 4.5177 - val_loss: 70.4704 - val_mae: 6.8738\n",
      "Epoch 16/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 4.5510 - mae: 2.133 - ETA: 0s - loss: 35.7468 - mae: 4.15 - ETA: 0s - loss: 44.0481 - mae: 4.43 - ETA: 0s - loss: 41.4599 - mae: 4.38 - ETA: 0s - loss: 36.1219 - mae: 4.15 - 0s 1ms/step - loss: 35.0591 - mae: 4.1092 - val_loss: 64.6277 - val_mae: 5.1441\n",
      "Epoch 17/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 20.6746 - mae: 4.54 - ETA: 0s - loss: 57.2768 - mae: 5.80 - ETA: 0s - loss: 45.7676 - mae: 5.02 - ETA: 0s - loss: 38.6150 - mae: 4.60 - ETA: 0s - loss: 35.0873 - mae: 4.48 - 0s 1ms/step - loss: 35.6612 - mae: 4.4761 - val_loss: 55.3988 - val_mae: 5.3862\n",
      "Epoch 18/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.7015 - mae: 1.304 - ETA: 0s - loss: 47.4738 - mae: 4.73 - ETA: 0s - loss: 40.1492 - mae: 4.56 - ETA: 0s - loss: 40.9408 - mae: 4.56 - ETA: 0s - loss: 35.0441 - mae: 4.27 - 0s 1ms/step - loss: 33.4111 - mae: 4.1643 - val_loss: 54.9432 - val_mae: 4.9945\n",
      "Epoch 19/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.3777 - mae: 1.173 - ETA: 0s - loss: 14.5541 - mae: 3.00 - ETA: 0s - loss: 22.0482 - mae: 3.51 - ETA: 0s - loss: 23.5599 - mae: 3.71 - ETA: 0s - loss: 28.0989 - mae: 3.87 - 0s 1ms/step - loss: 32.3999 - mae: 4.1282 - val_loss: 61.5444 - val_mae: 6.6446\n",
      "Epoch 20/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 64.0087 - mae: 8.00 - ETA: 0s - loss: 24.1508 - mae: 3.93 - ETA: 0s - loss: 30.0972 - mae: 4.18 - ETA: 0s - loss: 29.4938 - mae: 4.10 - ETA: 0s - loss: 33.9679 - mae: 4.32 - 0s 1ms/step - loss: 32.4964 - mae: 4.2195 - val_loss: 59.7849 - val_mae: 4.6983\n",
      "Epoch 21/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.2282 - mae: 0.477 - ETA: 0s - loss: 25.7302 - mae: 3.71 - ETA: 0s - loss: 33.7398 - mae: 4.03 - ETA: 0s - loss: 30.3791 - mae: 3.96 - ETA: 0s - loss: 31.7421 - mae: 4.01 - 0s 1ms/step - loss: 33.5832 - mae: 4.1491 - val_loss: 51.0536 - val_mae: 4.5770\n",
      "Epoch 22/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.0021 - mae: 0.045 - ETA: 0s - loss: 25.7088 - mae: 3.97 - ETA: 0s - loss: 25.1792 - mae: 3.78 - ETA: 0s - loss: 34.1470 - mae: 4.23 - ETA: 0s - loss: 32.9641 - mae: 4.19 - 0s 1ms/step - loss: 30.9240 - mae: 4.0685 - val_loss: 58.6148 - val_mae: 5.1185\n",
      "Epoch 23/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 22.3270 - mae: 4.72 - ETA: 0s - loss: 34.6694 - mae: 4.24 - ETA: 0s - loss: 31.1465 - mae: 4.07 - ETA: 0s - loss: 32.8884 - mae: 4.16 - ETA: 0s - loss: 32.9825 - mae: 4.15 - 0s 1ms/step - loss: 31.8229 - mae: 4.0448 - val_loss: 62.9385 - val_mae: 6.8570\n",
      "Epoch 24/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.2029 - mae: 1.096 - ETA: 0s - loss: 36.9667 - mae: 4.14 - ETA: 0s - loss: 31.8613 - mae: 4.22 - ETA: 0s - loss: 29.8568 - mae: 4.09 - ETA: 0s - loss: 28.1758 - mae: 3.95 - 0s 1ms/step - loss: 28.8464 - mae: 4.0084 - val_loss: 53.6215 - val_mae: 4.8959\n",
      "Epoch 25/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.7022 - mae: 0.838 - ETA: 0s - loss: 40.4982 - mae: 4.60 - ETA: 0s - loss: 33.6932 - mae: 4.24 - ETA: 0s - loss: 31.8450 - mae: 4.08 - ETA: 0s - loss: 32.2764 - mae: 4.09 - ETA: 0s - loss: 30.3004 - mae: 4.01 - 0s 1ms/step - loss: 29.9218 - mae: 3.9859 - val_loss: 78.6166 - val_mae: 5.7570\n",
      "Epoch 26/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 14.9822 - mae: 3.87 - ETA: 0s - loss: 15.7509 - mae: 3.00 - ETA: 0s - loss: 18.5204 - mae: 3.16 - ETA: 0s - loss: 21.7125 - mae: 3.42 - ETA: 0s - loss: 27.8198 - mae: 3.90 - 0s 1ms/step - loss: 27.2233 - mae: 3.9039 - val_loss: 46.4725 - val_mae: 4.9843\n",
      "Epoch 27/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 131.0664 - mae: 11.44 - ETA: 0s - loss: 25.3471 - mae: 3.8582 - ETA: 0s - loss: 24.4954 - mae: 3.54 - ETA: 0s - loss: 26.9100 - mae: 3.72 - ETA: 0s - loss: 26.7546 - mae: 3.73 - 0s 1ms/step - loss: 27.5720 - mae: 3.7980 - val_loss: 48.5605 - val_mae: 5.3867\n",
      "Epoch 28/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.9902 - mae: 1.729 - ETA: 0s - loss: 16.5869 - mae: 3.21 - ETA: 0s - loss: 26.5699 - mae: 3.58 - ETA: 0s - loss: 27.1538 - mae: 3.76 - ETA: 0s - loss: 26.2527 - mae: 3.71 - 0s 1ms/step - loss: 28.0038 - mae: 3.8110 - val_loss: 47.3974 - val_mae: 4.4422\n",
      "Epoch 29/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 40.3817 - mae: 6.35 - ETA: 0s - loss: 33.0866 - mae: 3.57 - ETA: 0s - loss: 26.7728 - mae: 3.45 - ETA: 0s - loss: 26.1036 - mae: 3.48 - ETA: 0s - loss: 26.7822 - mae: 3.62 - 0s 1ms/step - loss: 28.5241 - mae: 3.7249 - val_loss: 49.4694 - val_mae: 5.7278\n",
      "Epoch 30/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.0169 - mae: 3.002 - ETA: 0s - loss: 28.4410 - mae: 3.81 - ETA: 0s - loss: 21.2088 - mae: 3.27 - ETA: 0s - loss: 23.2634 - mae: 3.32 - ETA: 0s - loss: 25.8491 - mae: 3.53 - 0s 1ms/step - loss: 27.3192 - mae: 3.6423 - val_loss: 47.7716 - val_mae: 4.5531\n",
      "Epoch 31/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.3015 - mae: 2.881 - ETA: 0s - loss: 25.5932 - mae: 3.52 - ETA: 0s - loss: 26.8261 - mae: 3.68 - ETA: 0s - loss: 23.4584 - mae: 3.50 - ETA: 0s - loss: 26.1348 - mae: 3.73 - 0s 1ms/step - loss: 25.4282 - mae: 3.7402 - val_loss: 38.0206 - val_mae: 4.4789\n",
      "Epoch 32/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.9952 - mae: 3.161 - ETA: 0s - loss: 18.6218 - mae: 3.21 - ETA: 0s - loss: 23.3338 - mae: 3.58 - ETA: 0s - loss: 23.4996 - mae: 3.66 - ETA: 0s - loss: 23.5028 - mae: 3.59 - 0s 975us/step - loss: 24.1761 - mae: 3.6604 - val_loss: 59.4417 - val_mae: 6.6543\n",
      "Epoch 33/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 36.1933 - mae: 6.01 - ETA: 0s - loss: 19.2289 - mae: 3.19 - ETA: 0s - loss: 23.5282 - mae: 3.65 - ETA: 0s - loss: 21.1630 - mae: 3.47 - ETA: 0s - loss: 24.6508 - mae: 3.56 - 0s 1ms/step - loss: 25.7434 - mae: 3.6916 - val_loss: 45.0982 - val_mae: 4.8733\n",
      "Epoch 34/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.4837 - mae: 0.695 - ETA: 0s - loss: 40.1295 - mae: 4.63 - ETA: 0s - loss: 28.3051 - mae: 3.84 - ETA: 0s - loss: 24.2561 - mae: 3.60 - ETA: 0s - loss: 24.7747 - mae: 3.57 - 0s 1ms/step - loss: 23.5692 - mae: 3.5636 - val_loss: 43.4630 - val_mae: 4.7342\n",
      "Epoch 35/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 16.7407 - mae: 4.09 - ETA: 0s - loss: 30.2644 - mae: 4.10 - ETA: 0s - loss: 29.0878 - mae: 3.91 - ETA: 0s - loss: 24.8252 - mae: 3.61 - ETA: 0s - loss: 23.4386 - mae: 3.56 - ETA: 0s - loss: 24.3584 - mae: 3.61 - 0s 1ms/step - loss: 24.1111 - mae: 3.6081 - val_loss: 41.7209 - val_mae: 4.3636\n",
      "Epoch 36/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.1617 - mae: 1.470 - ETA: 0s - loss: 21.0337 - mae: 3.03 - ETA: 0s - loss: 22.8268 - mae: 3.30 - ETA: 0s - loss: 23.9308 - mae: 3.37 - ETA: 0s - loss: 24.7258 - mae: 3.50 - 0s 975us/step - loss: 23.9190 - mae: 3.4336 - val_loss: 44.4462 - val_mae: 4.2208\n",
      "Epoch 37/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.0603 - mae: 3.010 - ETA: 0s - loss: 20.2920 - mae: 3.18 - ETA: 0s - loss: 20.1786 - mae: 3.26 - ETA: 0s - loss: 20.2452 - mae: 3.34 - ETA: 0s - loss: 21.3420 - mae: 3.40 - 0s 1ms/step - loss: 21.1347 - mae: 3.3677 - val_loss: 45.4315 - val_mae: 4.5001\n",
      "Epoch 38/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.1546 - mae: 0.393 - ETA: 0s - loss: 29.1343 - mae: 3.51 - ETA: 0s - loss: 23.3048 - mae: 3.25 - ETA: 0s - loss: 20.8069 - mae: 3.19 - ETA: 0s - loss: 21.0644 - mae: 3.22 - 0s 1ms/step - loss: 20.8355 - mae: 3.2621 - val_loss: 32.3972 - val_mae: 4.2374\n",
      "Epoch 39/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.1542 - mae: 3.025 - ETA: 0s - loss: 18.4335 - mae: 3.22 - ETA: 0s - loss: 24.4296 - mae: 3.51 - ETA: 0s - loss: 24.0179 - mae: 3.57 - ETA: 0s - loss: 21.9884 - mae: 3.43 - 0s 1ms/step - loss: 22.0688 - mae: 3.4561 - val_loss: 52.7170 - val_mae: 4.7931\n",
      "Epoch 40/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 10.8846 - mae: 3.29 - ETA: 0s - loss: 22.8138 - mae: 3.67 - ETA: 0s - loss: 23.2094 - mae: 3.48 - ETA: 0s - loss: 23.4580 - mae: 3.54 - ETA: 0s - loss: 21.6520 - mae: 3.46 - 0s 1ms/step - loss: 21.3752 - mae: 3.4481 - val_loss: 37.7936 - val_mae: 3.9822\n",
      "Epoch 41/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.1613 - mae: 1.077 - ETA: 0s - loss: 14.7132 - mae: 3.00 - ETA: 0s - loss: 19.0710 - mae: 3.28 - ETA: 0s - loss: 20.7684 - mae: 3.33 - ETA: 0s - loss: 19.4854 - mae: 3.26 - 0s 1ms/step - loss: 22.2884 - mae: 3.4390 - val_loss: 52.1442 - val_mae: 4.4643\n",
      "Epoch 42/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.6646 - mae: 3.108 - ETA: 0s - loss: 24.0129 - mae: 3.61 - ETA: 0s - loss: 25.0891 - mae: 3.56 - ETA: 0s - loss: 25.0286 - mae: 3.50 - ETA: 0s - loss: 22.4328 - mae: 3.36 - 0s 1ms/step - loss: 21.8385 - mae: 3.3396 - val_loss: 24.3036 - val_mae: 3.6675\n",
      "Epoch 43/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 7.2113 - mae: 2.685 - ETA: 0s - loss: 19.3008 - mae: 3.22 - ETA: 0s - loss: 21.2137 - mae: 3.30 - ETA: 0s - loss: 21.7139 - mae: 3.46 - ETA: 0s - loss: 22.3995 - mae: 3.58 - 0s 1ms/step - loss: 21.1151 - mae: 3.4969 - val_loss: 39.4375 - val_mae: 4.1830\n",
      "Epoch 44/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.1330 - mae: 0.364 - ETA: 0s - loss: 25.0437 - mae: 3.47 - ETA: 0s - loss: 17.8438 - mae: 2.99 - ETA: 0s - loss: 19.9131 - mae: 3.18 - ETA: 0s - loss: 20.1510 - mae: 3.18 - 0s 1ms/step - loss: 19.8905 - mae: 3.2145 - val_loss: 60.0783 - val_mae: 5.6392\n",
      "Epoch 45/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.5587 - mae: 0.747 - ETA: 0s - loss: 22.1932 - mae: 3.19 - ETA: 0s - loss: 21.7494 - mae: 3.33 - ETA: 0s - loss: 20.8743 - mae: 3.21 - ETA: 0s - loss: 18.8844 - mae: 3.15 - ETA: 0s - loss: 20.5777 - mae: 3.33 - 0s 1ms/step - loss: 20.4348 - mae: 3.3141 - val_loss: 30.8226 - val_mae: 4.0084\n",
      "Epoch 46/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 10.7370 - mae: 3.27 - ETA: 0s - loss: 18.4508 - mae: 3.22 - ETA: 0s - loss: 19.0251 - mae: 3.26 - ETA: 0s - loss: 20.4601 - mae: 3.32 - ETA: 0s - loss: 19.7876 - mae: 3.25 - ETA: 0s - loss: 19.0130 - mae: 3.21 - 0s 1ms/step - loss: 19.1408 - mae: 3.2347 - val_loss: 45.1077 - val_mae: 5.1738\n",
      "Epoch 47/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.9032 - mae: 0.950 - ETA: 0s - loss: 14.5754 - mae: 3.01 - ETA: 0s - loss: 18.8898 - mae: 3.24 - ETA: 0s - loss: 16.8545 - mae: 3.08 - ETA: 0s - loss: 18.5943 - mae: 3.10 - ETA: 0s - loss: 19.4572 - mae: 3.20 - 0s 1ms/step - loss: 19.7305 - mae: 3.2371 - val_loss: 49.1531 - val_mae: 4.5236\n",
      "Epoch 48/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0518 - mae: 1.432 - ETA: 0s - loss: 15.1969 - mae: 2.88 - ETA: 0s - loss: 14.0172 - mae: 2.80 - ETA: 0s - loss: 19.0171 - mae: 3.12 - ETA: 0s - loss: 18.4910 - mae: 3.17 - 0s 1ms/step - loss: 19.4939 - mae: 3.1978 - val_loss: 60.2716 - val_mae: 4.9868\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - ETA: 0s - loss: 0.0106 - mae: 0.102 - ETA: 0s - loss: 12.0958 - mae: 2.58 - ETA: 0s - loss: 16.4422 - mae: 2.89 - ETA: 0s - loss: 19.6538 - mae: 3.26 - ETA: 0s - loss: 20.5517 - mae: 3.30 - ETA: 0s - loss: 21.6605 - mae: 3.30 - 0s 1ms/step - loss: 20.7167 - mae: 3.2375 - val_loss: 37.8581 - val_mae: 3.9375\n",
      "Epoch 50/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.8597 - mae: 0.927 - ETA: 0s - loss: 23.6377 - mae: 3.45 - ETA: 0s - loss: 24.5882 - mae: 3.65 - ETA: 0s - loss: 21.5746 - mae: 3.39 - ETA: 0s - loss: 20.3286 - mae: 3.31 - 0s 1ms/step - loss: 19.9071 - mae: 3.2817 - val_loss: 62.4597 - val_mae: 4.9708\n",
      "Epoch 51/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.6007 - mae: 0.775 - ETA: 0s - loss: 11.7887 - mae: 2.63 - ETA: 0s - loss: 13.3356 - mae: 2.74 - ETA: 0s - loss: 16.5776 - mae: 3.00 - ETA: 0s - loss: 17.5214 - mae: 3.09 - ETA: 0s - loss: 17.7929 - mae: 3.20 - 0s 1ms/step - loss: 17.5613 - mae: 3.1851 - val_loss: 32.7913 - val_mae: 3.7241\n",
      "Epoch 52/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.8624 - mae: 0.928 - ETA: 0s - loss: 27.3420 - mae: 3.89 - ETA: 0s - loss: 19.0095 - mae: 3.29 - ETA: 0s - loss: 21.0074 - mae: 3.39 - ETA: 0s - loss: 20.1386 - mae: 3.38 - 0s 1ms/step - loss: 19.2377 - mae: 3.3214 - val_loss: 29.9497 - val_mae: 3.6051\n",
      "Epoch 53/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 19.0977 - mae: 4.37 - ETA: 0s - loss: 27.7104 - mae: 3.45 - ETA: 0s - loss: 24.9652 - mae: 3.51 - ETA: 0s - loss: 22.4285 - mae: 3.43 - ETA: 0s - loss: 21.7996 - mae: 3.38 - 0s 1ms/step - loss: 20.5610 - mae: 3.2725 - val_loss: 38.9967 - val_mae: 4.1814\n",
      "Epoch 54/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.7897 - mae: 1.670 - ETA: 0s - loss: 24.5290 - mae: 3.44 - ETA: 0s - loss: 22.5430 - mae: 3.22 - ETA: 0s - loss: 19.8278 - mae: 3.13 - ETA: 0s - loss: 20.1334 - mae: 3.22 - 0s 1ms/step - loss: 20.1612 - mae: 3.2171 - val_loss: 26.4036 - val_mae: 3.9722\n",
      "Epoch 55/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 16.5294 - mae: 4.06 - ETA: 0s - loss: 18.9748 - mae: 3.25 - ETA: 0s - loss: 20.1124 - mae: 3.27 - ETA: 0s - loss: 18.9678 - mae: 3.19 - ETA: 0s - loss: 18.0381 - mae: 3.13 - 0s 1ms/step - loss: 20.0637 - mae: 3.2240 - val_loss: 27.0080 - val_mae: 4.1963\n",
      "Epoch 56/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.2784 - mae: 1.130 - ETA: 0s - loss: 14.1933 - mae: 2.91 - ETA: 0s - loss: 20.9522 - mae: 3.41 - ETA: 0s - loss: 16.8719 - mae: 3.03 - ETA: 0s - loss: 19.5771 - mae: 3.21 - 0s 1ms/step - loss: 19.7808 - mae: 3.2384 - val_loss: 24.4141 - val_mae: 3.4706\n",
      "Epoch 57/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.7743 - mae: 1.332 - ETA: 0s - loss: 17.4708 - mae: 3.25 - ETA: 0s - loss: 16.1658 - mae: 3.09 - ETA: 0s - loss: 17.5074 - mae: 3.19 - ETA: 0s - loss: 16.9471 - mae: 3.15 - 0s 1ms/step - loss: 17.3575 - mae: 3.1720 - val_loss: 24.2291 - val_mae: 3.3245\n",
      "Epoch 58/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.2238 - mae: 3.037 - ETA: 0s - loss: 17.4062 - mae: 3.09 - ETA: 0s - loss: 17.8565 - mae: 3.01 - ETA: 0s - loss: 19.6836 - mae: 3.27 - ETA: 0s - loss: 17.2426 - mae: 3.07 - 0s 1ms/step - loss: 18.3290 - mae: 3.1296 - val_loss: 36.4952 - val_mae: 3.8468\n",
      "Epoch 59/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3282 - mae: 1.525 - ETA: 0s - loss: 18.6281 - mae: 3.17 - ETA: 0s - loss: 19.8476 - mae: 3.18 - ETA: 0s - loss: 17.6605 - mae: 3.01 - ETA: 0s - loss: 16.7624 - mae: 2.91 - 0s 1ms/step - loss: 16.7524 - mae: 2.9533 - val_loss: 55.5662 - val_mae: 4.7076\n",
      "Epoch 60/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 3.1453 - mae: 1.773 - ETA: 0s - loss: 11.6921 - mae: 2.44 - ETA: 0s - loss: 13.2097 - mae: 2.71 - ETA: 0s - loss: 14.8519 - mae: 2.84 - ETA: 0s - loss: 17.0532 - mae: 3.00 - ETA: 0s - loss: 17.2692 - mae: 2.97 - 0s 1ms/step - loss: 17.8785 - mae: 3.0246 - val_loss: 28.4922 - val_mae: 4.2526\n",
      "Epoch 61/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 20.9243 - mae: 4.57 - ETA: 0s - loss: 16.6394 - mae: 3.00 - ETA: 0s - loss: 17.6330 - mae: 3.24 - ETA: 0s - loss: 15.9127 - mae: 3.06 - ETA: 0s - loss: 17.1528 - mae: 3.12 - ETA: 0s - loss: 18.0834 - mae: 3.21 - 0s 1ms/step - loss: 18.3126 - mae: 3.2393 - val_loss: 29.2017 - val_mae: 3.7432\n",
      "Epoch 62/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.9882 - mae: 1.410 - ETA: 0s - loss: 16.4193 - mae: 2.99 - ETA: 0s - loss: 18.3638 - mae: 3.11 - ETA: 0s - loss: 19.7956 - mae: 3.26 - ETA: 0s - loss: 17.7573 - mae: 3.04 - ETA: 0s - loss: 18.5858 - mae: 3.13 - 0s 1ms/step - loss: 18.7667 - mae: 3.1424 - val_loss: 27.3180 - val_mae: 3.3842\n",
      "Epoch 63/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 14.9290 - mae: 3.86 - ETA: 0s - loss: 15.6842 - mae: 2.93 - ETA: 0s - loss: 17.8682 - mae: 3.25 - ETA: 0s - loss: 16.9506 - mae: 3.15 - ETA: 0s - loss: 17.7765 - mae: 3.13 - ETA: 0s - loss: 17.4937 - mae: 3.11 - 0s 1ms/step - loss: 17.3410 - mae: 3.1062 - val_loss: 25.6970 - val_mae: 3.4939\n",
      "Epoch 64/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.0588 - mae: 1.434 - ETA: 0s - loss: 12.6785 - mae: 2.74 - ETA: 0s - loss: 16.4378 - mae: 2.96 - ETA: 0s - loss: 18.5951 - mae: 3.16 - ETA: 0s - loss: 18.7384 - mae: 3.15 - 0s 1ms/step - loss: 17.9150 - mae: 3.1082 - val_loss: 32.2675 - val_mae: 3.9074\n",
      "Epoch 65/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.0265 - mae: 1.013 - ETA: 0s - loss: 21.7743 - mae: 3.33 - ETA: 0s - loss: 17.7148 - mae: 3.09 - ETA: 0s - loss: 18.2251 - mae: 3.04 - ETA: 0s - loss: 19.1279 - mae: 3.15 - ETA: 0s - loss: 18.0716 - mae: 3.08 - 0s 1ms/step - loss: 17.5007 - mae: 3.0382 - val_loss: 28.3771 - val_mae: 3.5420\n",
      "Epoch 66/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.3566 - mae: 0.597 - ETA: 0s - loss: 23.2206 - mae: 3.42 - ETA: 0s - loss: 18.8535 - mae: 3.12 - ETA: 0s - loss: 17.3489 - mae: 3.08 - ETA: 0s - loss: 17.3126 - mae: 3.06 - 0s 1ms/step - loss: 18.1961 - mae: 3.1592 - val_loss: 25.6591 - val_mae: 3.4547\n",
      "Epoch 67/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 30.4145 - mae: 5.51 - ETA: 0s - loss: 18.9923 - mae: 3.36 - ETA: 0s - loss: 19.5379 - mae: 3.32 - ETA: 0s - loss: 17.9033 - mae: 3.12 - ETA: 0s - loss: 16.4110 - mae: 3.03 - 0s 1ms/step - loss: 16.2220 - mae: 3.0261 - val_loss: 30.3629 - val_mae: 4.0858\n",
      "Epoch 68/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 27.1742 - mae: 5.21 - ETA: 0s - loss: 12.0996 - mae: 2.71 - ETA: 0s - loss: 14.0139 - mae: 2.92 - ETA: 0s - loss: 17.6872 - mae: 3.11 - ETA: 0s - loss: 18.6816 - mae: 3.13 - 0s 1ms/step - loss: 17.4458 - mae: 3.0199 - val_loss: 34.2750 - val_mae: 4.6189\n",
      "Epoch 69/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.8829 - mae: 2.980 - ETA: 0s - loss: 11.8491 - mae: 2.72 - ETA: 0s - loss: 16.3543 - mae: 3.01 - ETA: 0s - loss: 18.7126 - mae: 3.04 - ETA: 0s - loss: 17.7669 - mae: 3.00 - 0s 1ms/step - loss: 18.3372 - mae: 3.0694 - val_loss: 36.6154 - val_mae: 4.1536\n",
      "Epoch 70/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 24.9123 - mae: 4.99 - ETA: 0s - loss: 11.0588 - mae: 2.50 - ETA: 0s - loss: 11.9233 - mae: 2.56 - ETA: 0s - loss: 14.1220 - mae: 2.87 - ETA: 0s - loss: 15.6512 - mae: 2.92 - 0s 1ms/step - loss: 17.6556 - mae: 3.0600 - val_loss: 25.9084 - val_mae: 3.5766\n",
      "Epoch 71/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.0044 - mae: 0.066 - ETA: 0s - loss: 15.6624 - mae: 2.93 - ETA: 0s - loss: 19.3270 - mae: 3.22 - ETA: 0s - loss: 18.0020 - mae: 3.07 - ETA: 0s - loss: 17.8414 - mae: 3.15 - 0s 1ms/step - loss: 17.1794 - mae: 3.0898 - val_loss: 27.4067 - val_mae: 3.8714\n",
      "Epoch 72/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.4517 - mae: 2.907 - ETA: 0s - loss: 11.9695 - mae: 2.54 - ETA: 0s - loss: 13.9777 - mae: 2.75 - ETA: 0s - loss: 16.3167 - mae: 2.96 - ETA: 0s - loss: 16.5062 - mae: 2.93 - 0s 943us/step - loss: 16.6966 - mae: 2.9492 - val_loss: 34.8623 - val_mae: 4.2765\n",
      "Epoch 73/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 26.7704 - mae: 5.17 - ETA: 0s - loss: 12.4655 - mae: 2.67 - ETA: 0s - loss: 14.3747 - mae: 2.73 - ETA: 0s - loss: 12.6959 - mae: 2.64 - ETA: 0s - loss: 16.4615 - mae: 2.86 - 0s 1000us/step - loss: 15.8876 - mae: 2.8537 - val_loss: 23.1843 - val_mae: 3.5009\n",
      "Epoch 74/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.8213 - mae: 0.906 - ETA: 0s - loss: 15.4504 - mae: 3.08 - ETA: 0s - loss: 14.7023 - mae: 2.86 - ETA: 0s - loss: 14.3664 - mae: 2.81 - ETA: 0s - loss: 15.8150 - mae: 2.90 - 0s 1ms/step - loss: 15.8005 - mae: 2.9311 - val_loss: 23.9288 - val_mae: 3.3977\n",
      "Epoch 75/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.3606 - mae: 0.600 - ETA: 0s - loss: 19.1944 - mae: 3.18 - ETA: 0s - loss: 16.7917 - mae: 2.88 - ETA: 0s - loss: 16.7553 - mae: 2.98 - ETA: 0s - loss: 16.4571 - mae: 3.00 - 0s 1ms/step - loss: 16.2108 - mae: 2.9924 - val_loss: 52.0159 - val_mae: 4.5609\n",
      "Epoch 76/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 21.4322 - mae: 4.62 - ETA: 0s - loss: 27.5007 - mae: 3.61 - ETA: 0s - loss: 18.7309 - mae: 3.05 - ETA: 0s - loss: 16.5673 - mae: 2.94 - ETA: 0s - loss: 17.6273 - mae: 3.04 - 0s 1ms/step - loss: 18.1169 - mae: 3.0772 - val_loss: 24.4955 - val_mae: 3.3972\n",
      "Epoch 77/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.7656e-05 - mae: 0.004 - ETA: 0s - loss: 14.4785 - mae: 2.6634  - ETA: 0s - loss: 14.4269 - mae: 2.77 - ETA: 0s - loss: 16.4387 - mae: 2.98 - ETA: 0s - loss: 16.4508 - mae: 2.99 - 0s 1ms/step - loss: 16.2722 - mae: 2.9568 - val_loss: 28.9357 - val_mae: 3.7724\n",
      "Epoch 78/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.8161 - mae: 0.903 - ETA: 0s - loss: 16.5873 - mae: 2.57 - ETA: 0s - loss: 18.6203 - mae: 2.84 - ETA: 0s - loss: 17.5656 - mae: 2.93 - ETA: 0s - loss: 16.2049 - mae: 2.89 - 0s 1ms/step - loss: 17.5085 - mae: 2.9592 - val_loss: 47.6728 - val_mae: 4.4110\n",
      "Epoch 79/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 16.3859 - mae: 4.04 - ETA: 0s - loss: 12.0337 - mae: 2.61 - ETA: 0s - loss: 12.9312 - mae: 2.76 - ETA: 0s - loss: 13.6679 - mae: 2.83 - ETA: 0s - loss: 15.7711 - mae: 2.98 - 0s 1ms/step - loss: 15.2332 - mae: 2.9404 - val_loss: 27.8907 - val_mae: 3.7825\n",
      "Epoch 80/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 31.4132 - mae: 5.60 - ETA: 0s - loss: 9.9587 - mae: 2.6363 - ETA: 0s - loss: 13.6153 - mae: 2.74 - ETA: 0s - loss: 15.3195 - mae: 2.81 - ETA: 0s - loss: 15.8161 - mae: 2.87 - 0s 1ms/step - loss: 15.0380 - mae: 2.8416 - val_loss: 40.1542 - val_mae: 4.1506\n",
      "Epoch 81/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.1351 - mae: 0.367 - ETA: 0s - loss: 10.0972 - mae: 2.37 - ETA: 0s - loss: 15.8949 - mae: 2.91 - ETA: 0s - loss: 16.7046 - mae: 3.00 - ETA: 0s - loss: 17.6057 - mae: 3.14 - 0s 1ms/step - loss: 16.1236 - mae: 2.9898 - val_loss: 42.1299 - val_mae: 4.1627\n",
      "Epoch 82/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.2581 - mae: 3.042 - ETA: 0s - loss: 20.7474 - mae: 3.12 - ETA: 0s - loss: 18.4264 - mae: 3.11 - ETA: 0s - loss: 16.5995 - mae: 3.01 - ETA: 0s - loss: 15.9074 - mae: 2.98 - 0s 1ms/step - loss: 15.7116 - mae: 2.9637 - val_loss: 24.2382 - val_mae: 3.5308\n",
      "Epoch 83/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.3849 - mae: 0.620 - ETA: 0s - loss: 9.3799 - mae: 2.420 - ETA: 0s - loss: 12.9740 - mae: 2.49 - ETA: 0s - loss: 15.7696 - mae: 2.78 - ETA: 0s - loss: 17.2605 - mae: 2.94 - 0s 1ms/step - loss: 15.4309 - mae: 2.8287 - val_loss: 30.0111 - val_mae: 3.9160\n",
      "Epoch 84/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.6878 - mae: 0.829 - ETA: 0s - loss: 11.1889 - mae: 2.57 - ETA: 0s - loss: 14.0460 - mae: 2.86 - ETA: 0s - loss: 14.1125 - mae: 2.90 - ETA: 0s - loss: 14.8258 - mae: 2.94 - 0s 1ms/step - loss: 14.5150 - mae: 2.9062 - val_loss: 36.6530 - val_mae: 3.9630\n",
      "Epoch 85/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 62.9636 - mae: 7.93 - ETA: 0s - loss: 17.2091 - mae: 3.02 - ETA: 0s - loss: 15.7930 - mae: 2.89 - ETA: 0s - loss: 16.9166 - mae: 2.98 - ETA: 0s - loss: 14.7595 - mae: 2.78 - 0s 1ms/step - loss: 15.6088 - mae: 2.9044 - val_loss: 33.0293 - val_mae: 4.2885\n",
      "Epoch 86/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.7922 - mae: 2.965 - ETA: 0s - loss: 17.2848 - mae: 2.92 - ETA: 0s - loss: 17.9316 - mae: 2.99 - ETA: 0s - loss: 15.6321 - mae: 2.70 - ETA: 0s - loss: 17.1728 - mae: 2.88 - 0s 1ms/step - loss: 17.1884 - mae: 2.9025 - val_loss: 39.1160 - val_mae: 4.0612\n",
      "Epoch 87/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.2690e-04 - mae: 0.011 - ETA: 0s - loss: 12.8794 - mae: 2.6819  - ETA: 0s - loss: 11.5160 - mae: 2.60 - ETA: 0s - loss: 14.8160 - mae: 2.75 - ETA: 0s - loss: 15.0263 - mae: 2.85 - 0s 1ms/step - loss: 14.7538 - mae: 2.8394 - val_loss: 61.3792 - val_mae: 5.0929\n",
      "Epoch 88/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 33.6975 - mae: 5.80 - ETA: 0s - loss: 14.9425 - mae: 2.64 - ETA: 0s - loss: 14.9150 - mae: 2.76 - ETA: 0s - loss: 15.9034 - mae: 2.85 - ETA: 0s - loss: 15.2836 - mae: 2.85 - 0s 1ms/step - loss: 15.0183 - mae: 2.8066 - val_loss: 34.0062 - val_mae: 4.0877\n",
      "Epoch 89/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 38.4266 - mae: 6.19 - ETA: 0s - loss: 16.5728 - mae: 2.82 - ETA: 0s - loss: 12.3817 - mae: 2.48 - ETA: 0s - loss: 13.4059 - mae: 2.66 - ETA: 0s - loss: 13.3506 - mae: 2.66 - 0s 1ms/step - loss: 13.9044 - mae: 2.7316 - val_loss: 37.0678 - val_mae: 3.9631\n",
      "Epoch 90/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 8.8185 - mae: 2.969 - ETA: 0s - loss: 16.7747 - mae: 3.20 - ETA: 0s - loss: 15.2788 - mae: 3.02 - ETA: 0s - loss: 14.3482 - mae: 2.87 - ETA: 0s - loss: 15.4666 - mae: 2.93 - 0s 1ms/step - loss: 15.9413 - mae: 3.0052 - val_loss: 32.8787 - val_mae: 3.9588\n",
      "Epoch 91/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 81.7244 - mae: 9.04 - ETA: 0s - loss: 17.5488 - mae: 2.87 - ETA: 0s - loss: 14.9011 - mae: 2.76 - ETA: 0s - loss: 17.7193 - mae: 2.97 - ETA: 0s - loss: 15.8631 - mae: 2.84 - 0s 1ms/step - loss: 15.1354 - mae: 2.8207 - val_loss: 37.2583 - val_mae: 3.8772\n",
      "Epoch 92/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.1174 - mae: 0.342 - ETA: 0s - loss: 9.9279 - mae: 2.313 - ETA: 0s - loss: 10.8543 - mae: 2.39 - ETA: 0s - loss: 12.9856 - mae: 2.58 - ETA: 0s - loss: 14.6138 - mae: 2.75 - 0s 1ms/step - loss: 15.5986 - mae: 2.8523 - val_loss: 32.7152 - val_mae: 4.3870\n",
      "Epoch 93/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 1.2260 - mae: 1.107 - ETA: 0s - loss: 14.0009 - mae: 2.74 - ETA: 0s - loss: 12.8928 - mae: 2.61 - ETA: 0s - loss: 12.6856 - mae: 2.65 - ETA: 0s - loss: 15.4901 - mae: 2.83 - 0s 1ms/step - loss: 15.0076 - mae: 2.8172 - val_loss: 41.4956 - val_mae: 5.2790\n",
      "Epoch 94/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 4.5162 - mae: 2.125 - ETA: 0s - loss: 14.7327 - mae: 2.71 - ETA: 0s - loss: 16.2317 - mae: 2.77 - ETA: 0s - loss: 14.3831 - mae: 2.67 - ETA: 0s - loss: 14.4833 - mae: 2.69 - 0s 1ms/step - loss: 14.3559 - mae: 2.7165 - val_loss: 39.0832 - val_mae: 4.2111\n",
      "Epoch 95/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 4.2151 - mae: 2.053 - ETA: 0s - loss: 13.9092 - mae: 2.81 - ETA: 0s - loss: 17.0072 - mae: 3.04 - ETA: 0s - loss: 16.8761 - mae: 3.05 - ETA: 0s - loss: 15.0664 - mae: 2.89 - 0s 1ms/step - loss: 15.7349 - mae: 2.9307 - val_loss: 26.7230 - val_mae: 3.4206\n",
      "Epoch 96/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 6.7605 - mae: 2.600 - ETA: 0s - loss: 9.7029 - mae: 2.358 - ETA: 0s - loss: 14.2893 - mae: 2.69 - ETA: 0s - loss: 14.0377 - mae: 2.77 - ETA: 0s - loss: 14.6440 - mae: 2.85 - 0s 1ms/step - loss: 14.8349 - mae: 2.8926 - val_loss: 56.4010 - val_mae: 4.8249\n",
      "Epoch 97/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.8744 - mae: 1.695 - ETA: 0s - loss: 9.2012 - mae: 2.241 - ETA: 0s - loss: 14.7330 - mae: 2.84 - ETA: 0s - loss: 14.7292 - mae: 2.72 - ETA: 0s - loss: 14.8904 - mae: 2.80 - 0s 996us/step - loss: 14.6775 - mae: 2.8007 - val_loss: 24.4414 - val_mae: 3.6289\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/283 [==============================] - ETA: 0s - loss: 4.9454 - mae: 2.223 - ETA: 0s - loss: 15.4281 - mae: 2.91 - ETA: 0s - loss: 14.5856 - mae: 2.88 - ETA: 0s - loss: 13.4252 - mae: 2.74 - ETA: 0s - loss: 15.5472 - mae: 2.92 - 0s 1ms/step - loss: 14.9826 - mae: 2.9040 - val_loss: 30.2468 - val_mae: 3.6946\n",
      "Epoch 99/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 9.7147 - mae: 3.116 - ETA: 0s - loss: 10.6345 - mae: 2.33 - ETA: 0s - loss: 12.5559 - mae: 2.65 - ETA: 0s - loss: 14.3202 - mae: 2.76 - ETA: 0s - loss: 13.4277 - mae: 2.63 - 0s 1ms/step - loss: 14.5701 - mae: 2.7394 - val_loss: 24.3315 - val_mae: 3.5392\n",
      "Epoch 100/100\n",
      "283/283 [==============================] - ETA: 0s - loss: 0.6393 - mae: 0.799 - ETA: 0s - loss: 13.5829 - mae: 2.64 - ETA: 0s - loss: 13.2485 - mae: 2.63 - ETA: 0s - loss: 16.2745 - mae: 2.84 - ETA: 0s - loss: 15.0765 - mae: 2.74 - 0s 1ms/step - loss: 15.1080 - mae: 2.7512 - val_loss: 29.8174 - val_mae: 3.9839\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(xTrain, yTrain, epochs=100, batch_size=1,validation_data=(xVal,yVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7165222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(history.history['mae'])\n",
    "epo=list(range(100))\n",
    "mae=np.array(history.history['mae']).reshape(100,1)\n",
    "plt.plot(epo,mae)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - ETA:  - 0s 49us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30.96131522982728, 4.224552631378174]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testData,testTarget)\n",
    "# [30.96131522982728, 4.224552631378174]\n",
    "# 약 422달러의 오차가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n",
      "(102, 13)\n",
      "(102, 1)\n"
     ]
    }
   ],
   "source": [
    "trainTargets=trainTargets.reshape(404,1)\n",
    "testTarget=testTarget.reshape(102,1)\n",
    "print(trainData.shape)\n",
    "print(trainTargets.shape)\n",
    "print(testData.shape)\n",
    "print(testTarget.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=trainData[:283]\n",
    "xVal=trainData[283:]\n",
    "yTrain=trainTargets[:283]\n",
    "yVal=trainTargets[283:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain=yTrain.reshape(283,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "yVal=yVal.reshape(121,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 13)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain.shape\n",
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2 tensorflow\n",
    "x=tf.placeholder(tf.float32, shape=[None, 13])\n",
    "y=tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(tf.random_normal([13,64]))\n",
    "b1=tf.Variable(tf.random_normal([64]))\n",
    "l1=tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.Variable(tf.random_normal([64,64]))\n",
    "b2=tf.Variable(tf.random_normal([64]))\n",
    "l2=tf.sigmoid(tf.matmul(l1,w2)+b2)\n",
    "\n",
    "w3=tf.Variable(tf.random_normal([64,1]))\n",
    "b3=tf.Variable(tf.random_normal([1]))\n",
    "hf=tf.matmul(l2,w3)+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 값과 예측값의 차이가 얼마인지 궁금해서 노드 추가함\n",
    "mae=tf.reduce_mean(abs(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.square(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 step: 451.35458\n",
      "1000 step: 59.718624\n",
      "2000 step: 59.117878\n",
      "3000 step: 58.951256\n",
      "4000 step: 58.870316\n",
      "5000 step: 58.82525\n",
      "6000 step: 58.77512\n",
      "7000 step: 58.74639\n",
      "8000 step: 58.726135\n",
      "9000 step: 58.711304\n",
      "10000 step: 58.70031\n",
      "11000 step: 58.69712\n",
      "12000 step: 58.692574\n",
      "13000 step: 58.681446\n",
      "14000 step: 58.6769\n",
      "15000 step: 58.680584\n",
      "16000 step: 58.674347\n",
      "17000 step: 58.67293\n",
      "18000 step: 58.671654\n",
      "19000 step: 58.670578\n",
      "20000 step: 58.669716\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    predict_hf=[]\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(20001):\n",
    "        _,cv,hv=sess.run([train,cost,hf], feed_dict={x:xTrain,y:yTrain})\n",
    "        predict_hf.append(hv)\n",
    "        if step%1000==0:\n",
    "            print(step,\"step:\",cv)\n",
    "    mv=sess.run([mae], feed_dict={x:xVal,y:yVal})\n",
    "    predict_hf.append(mv)\n",
    "#     pre_hf=np.array(predict_hf).reshape(121,1)\n",
    "#     val_range=list(range(len(pre_hf)))\n",
    "#     plt.plot(val_range, pre_hf/len(pre_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002,)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predict_hf).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
