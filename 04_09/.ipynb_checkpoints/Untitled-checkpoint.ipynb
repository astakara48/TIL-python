{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "표제어(Lemmatization) 추출: 서로 형태는 다르지만, root 단어를 가지고 비교해서, 전체적으로 단어의 개수를 줄이자.\n",
    "am, are, is, was, were .. => be(표제어)\n",
    "\n",
    "형태소 : stem(어간, 단어의 의미), affix(접사, 부가적 의미)\n",
    "형태소 파싱 : 어간, 접사를 분리하는 작업\n",
    "dog(독립형태소)\n",
    "dogs = dog(어간) + s(접사)\n",
    "\n",
    "WordNetLemmatizer : NLTK에 표제어 추출 도구\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('watched') # watched\n",
    "wnl.lemmatize('watched', 'v') # watch => 동사 원형 출력\n",
    "wnl.lemmatize('has', 'v')\n",
    "wnl.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출\n",
    "text='Python is an interpreted, high-level, general-purpose programming language.'\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric\n",
      "formal\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('electricical'))\n",
    "print(ps.stem('formalize'))\n",
    "# 구글 : 마틴 포터 or 포터스태머 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elect\n",
      "form\n"
     ]
    }
   ],
   "source": [
    "ls=LancasterStemmer()\n",
    "print(ls.stem('electricical'))\n",
    "print(ls.stem('formalize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어:stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n"
     ]
    }
   ],
   "source": [
    "sw=stopwords.words('english')\n",
    "ex=\"Family is not an important thing. It's everything\"\n",
    "wt=word_tokenize(ex)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in sw:\n",
    "        res.append(w)\n",
    "print(wt)\n",
    "print(res)\n",
    "\n",
    "# 자연어 처리작업할때 => 불용어 사전을 아예 따로 만드는 것도 좋음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최근', '코로나19로', '인한', '감염으로', '인해', '확진자', '및', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다', '.']\n",
      "['코로나19로', '감염으로', '인해', '확진자', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다', '.']\n"
     ]
    }
   ],
   "source": [
    "ex='최근 코로나19로 인한 감염으로 인해 확진자 및 사망자가 증가하고 있습니다. 코로나19를 이겨냅시다.'\n",
    "stop_words=\"인한 증가 최근 및\"\n",
    "stop_words=stop_words.split(\" \")\n",
    "wt=word_tokenize(ex)\n",
    "print(wt)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in stop_words:\n",
    "        res.append(w)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']\n",
      "{'python': 2, 'interpreted': 1, 'high-level': 1, 'general-purpose': 1, 'programming': 1, 'language': 2, 'created': 1, 'guido': 1, 'van': 1, 'rossum': 1, 'first': 1, 'released': 1, '1991': 1, 'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 2, 'readability': 1, 'notable': 1, 'use': 1, 'significant': 1, 'whitespace': 1, 'constructs': 1, 'object-oriented': 1, 'approach': 1, 'aim': 1, 'help': 1, 'programmers': 1, 'write': 1, 'clear': 1, 'logical': 1, 'small': 1, 'large-scale': 1, 'projects': 1}\n",
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "\n",
    "text=sent_tokenize(text)\n",
    "text\n",
    "\n",
    "voc={}\n",
    "sentences=[]\n",
    "# 모든 단어를 소문자, 불용어 제거, 길이가 2이하 제거\n",
    "#print(sw)\n",
    "for t in text:\n",
    "    words=word_tokenize(t)\n",
    "    res=[]\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word not in sw:\n",
    "            if len(word)>2:\n",
    "                res.append(word)\n",
    "                if word not in voc:\n",
    "                    voc[word]=0\n",
    "                voc[word]+=1\n",
    "    sentences.append(res)\n",
    "print(res)\n",
    "print(voc)\n",
    "print(sentences) #문단[문장1,문장2,문장3]\n",
    "# {'python':3, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('write', 1),\n",
       " ('whitespace', 1),\n",
       " ('van', 1),\n",
       " ('use', 1),\n",
       " ('small', 1),\n",
       " ('significant', 1),\n",
       " ('rossum', 1),\n",
       " ('released', 1),\n",
       " ('readability', 1),\n",
       " ('python', 2),\n",
       " ('projects', 1),\n",
       " ('programming', 1),\n",
       " ('programmers', 1),\n",
       " ('philosophy', 1),\n",
       " ('object-oriented', 1),\n",
       " ('notable', 1),\n",
       " ('logical', 1),\n",
       " ('large-scale', 1),\n",
       " ('language', 2),\n",
       " ('interpreted', 1),\n",
       " ('high-level', 1),\n",
       " ('help', 1),\n",
       " ('guido', 1),\n",
       " ('general-purpose', 1),\n",
       " ('first', 1),\n",
       " ('emphasizes', 1),\n",
       " ('design', 1),\n",
       " ('created', 1),\n",
       " ('constructs', 1),\n",
       " ('code', 2),\n",
       " ('clear', 1),\n",
       " ('approach', 1),\n",
       " ('aim', 1),\n",
       " ('1991', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=sorted(voc.items(), key=lambda x:x[0], reverse=True)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1, 'language': 2, 'code': 3}\n"
     ]
    }
   ],
   "source": [
    "wi={}\n",
    "i=0\n",
    "for w,f in vs:\n",
    "    if f>1:\n",
    "        i+=1\n",
    "        wi[w]=i\n",
    "print(wi) # 2회 이상 나온 단어들에 대한 인덱스를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a=wi.items()\n",
    "for w, i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'language', 'code']\n",
      "['code']\n",
      "{'python': 1, 'language': 2}\n"
     ]
    }
   ],
   "source": [
    "vocSize=2 # 가장 많이 언급된 2개 단어만 추출\n",
    "\n",
    "# index가 3번 이상인 단어는 제거\n",
    "wordFreq=[w for w, i in wi.items()] # w=key, i=value, wi.items()에서 w를 저장하라\n",
    "print(wordFreq)\n",
    "\n",
    "# 단어의 인덱스가 vocSize를 초과하는 단어 추출\n",
    "wordFreq=[w for w, i in wi.items() if i>vocSize]\n",
    "print(wordFreq)\n",
    "for w in wordFreq:\n",
    "    del wi[w]\n",
    "    \n",
    "print(wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV(out of voc, 단어집합에 없는 단어)\n",
    "영수 : 철수야 안녕? (입력 데이터, x)\n",
    "철수 : 영수야 안녕. (출력 데이터, y)\n",
    "....\n",
    "철수야 안녕? -> 모델 -> 응 너도 안녕.\n",
    "=> 영어, 한국어 모두 수치화 해줘야 함\n",
    "철수 안녕 ->         -> 응 너 안녕\n",
    "....\n",
    "철수 안녕 너 응 잘가 ....\n",
    "1    2    3  4  5 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt=Okt()\n",
    "tok=okt.morphs('나는 자연어 처리를 학습한다')\n",
    "# 원핫벡터 : 단어 집합을 벡터로 표현하는 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '학습': 5, '한다': 6}\n"
     ]
    }
   ],
   "source": [
    "w2i={}\n",
    "for v in tok:\n",
    "    if v not in w2i.keys():\n",
    "        w2i[v]=len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 자연어 -> 원핫 -> 0010000\n",
    "def ohe(w, w2i):\n",
    "    ohv=[0]*len(w2i)\n",
    "    index=w2i[w]\n",
    "    ohv[index]=1\n",
    "    return ohv\n",
    "print(ohe(\"자연어\",w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 원핫인코딩 :  to_categorical()\n",
    "text=\"데이터 분석은 판다스 최고야 판다스 곰이야\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다스': 1, '데이터': 2, '분석은': 3, '최고야': 4, '곰이야': 5}\n"
     ]
    }
   ],
   "source": [
    "tok=Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "print(tok.word_index)\n",
    "# 단어집합 : VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample='판다스 분석은 동물원에서 한다'\n",
    "enc=tok.texts_to_sequences([sample])[0]\n",
    "# 위에 저장한 tok에 저 문장을 검색해서 겹치는것 2개 인덱스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 분리(BPE)\n",
    "# 학습과정에서 사용되지 않은 단어가 테스트과정에서 입력이 되면 -> OOV => 제대로 모델이 동작안함\n",
    "# 입력되면 -> OOV 문제 => 제대로 모델이 동작 X\n",
    "\n",
    "# run-length 기법 aaaabbbaaaaa => a4b3a5 ex) bmp 파일\n",
    "# 퍼프만 트리(인코딩)를 이용한 압축 \n",
    "# a=> 101, b= 10, c=1101 ..... \n",
    "# BPE 압축 알고리즘 => 단어 분리에 응용이 됨 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
